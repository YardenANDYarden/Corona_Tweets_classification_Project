{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8fwlj7vYQBb",
        "outputId": "08a1254e-b560-46a6-be17-4f18c0487574"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# # ========================================\n",
        "# # 1) Connect to Google Drive\n",
        "# # ========================================\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# #MODEL_DIR = \"/content/drive/MyDrive/deep_learning\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "18LqQBe_YmqH",
        "outputId": "931b824f-dece-4508-fc3d-13e19f9c3f7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.55.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting optuna\n",
            "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.21.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.16.4-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.43)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.7)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.34.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.14.1)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Downloading evaluate-0.4.5-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.4-py3-none-any.whl (247 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m128.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, colorlog, nvidia-cusparse-cu12, nvidia-cudnn-cu12, alembic, optuna, nvidia-cusolver-cu12, evaluate\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed alembic-1.16.4 colorlog-6.9.0 evaluate-0.4.5 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 optuna-4.5.0\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# 2) Install required libraries\n",
        "# ========================================\n",
        "!pip install -U transformers datasets accelerate evaluate optuna wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRlucwrcZzDm",
        "outputId": "9c69dabf-2669-4fed-eed4-454b7b423a05",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.5)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.34.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_lUu73RwbbY",
        "outputId": "f0122806-cbdc-4de1-e8d3-11377ac7d6f9",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.55.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWSgKFUoYqzs"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# 3) Import libraries\n",
        "# ========================================\n",
        "import os, time, copy, tempfile, inspect\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import torch, torch.nn as nn\n",
        "from torch.quantization import quantize_dynamic\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
        "import os\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel, DataCollatorWithPadding\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQr3wGwNYsO7",
        "outputId": "e02d148c-01e6-440f-b209-4bfb1ee6d5c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Check if GPU is available and select device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBF3Ga4FY28M"
      },
      "outputs": [],
      "source": [
        "# === Load data ===\n",
        "#test_df = pd.read_csv('/content/drive/MyDrive/deep_learning/val_processed.csv', encoding='latin1')\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "# Navigate from model folder to data folder\n",
        "current_dir = Path.cwd()  # models/bert-base-uncased/\n",
        "models_dir = current_dir.parent  # models/\n",
        "project_root = models_dir.parent  # project root\n",
        "data_dir = models_dir / 'data'\n",
        "\n",
        "# Load data\n",
        "test_df = pd.read_csv(data_dir / 'test_processed.csv', encoding='latin1')\n",
        "\n",
        "print(f\"Test shape: {test_df.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df[\"label\"] = test_df[\"Sentiment\"].map(label2id)"
      ],
      "metadata": {
        "id": "HUgOIRMPMi5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_for_cardiffnlp(text):\n",
        "    if pd.isnull(text):\n",
        "        return \"\"\n",
        "\n",
        "    tokens = []\n",
        "    for t in text.split(\" \"):\n",
        "        if t.startswith(\"@\") and len(t) > 1:\n",
        "            tokens.append(\"@user\")\n",
        "        elif t.startswith(\"http\"):\n",
        "            tokens.append(\"http\")\n",
        "        else:\n",
        "            tokens.append(t)\n",
        "    text = \" \".join(tokens)\n",
        "\n",
        "    # Normalize common COVID variants to \"covid\"\n",
        "    text = re.sub(r\"\\b(coronaviruspandemic|covid[_\\s-]*2019|covid[_\\s-]*19|covid2019|coronavirus2019|coronavirus|corona)\\b\", \"covid\", text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Decode HTML entities\n",
        "    text = html.unescape(text)\n",
        "\n",
        "    # Normalize whitespace and repeated punctuation (optional)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    text = re.sub(r\"(\\.\\s*){2,}\", \". \", text)\n",
        "    text = re.sub(r\"([!?]){2,}\", r\"\\1\", text)\n",
        "    text = re.sub(r\"(\\?\\s+){2,}\", \"?\", text)\n",
        "    text = re.sub(r\"(\\!\\s+){2,}\", \"!\", text)\n",
        "\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "I54afjFfMjUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_CHECKPOINT = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "LABEL_COL = \"label\"\n",
        "BATCH_SIZE = 128\n",
        "NUM_WORKERS = 2\n",
        "PIN_MEMORY = True\n",
        "\n",
        "\n",
        "# ==== tokenizer + collator ====\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_CHECKPOINT, use_fast=True)\n",
        "collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# ==== Class Dataset ====\n",
        "class TextClsDataset(Dataset):\n",
        "    def __init__(self, df, text_col, label_col):\n",
        "        self.texts = df[text_col].astype(str).tolist()\n",
        "        self.labels = df[label_col].astype(int).tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        enc = tokenizer(\n",
        "            self.texts[idx],\n",
        "            truncation=True,\n",
        "            max_length=128,\n",
        "            return_tensors=None\n",
        "        )\n",
        "        enc[\"labels\"] = self.labels[idx]\n",
        "        return enc\n",
        "\n",
        "# prepere datasets\n",
        "test_ds_clean = TextClsDataset(test_df, text_col=\"ProcessedTweet\",        label_col=LABEL_COL)\n",
        "\n",
        "# DataLoaders\n",
        "test_loader_clean = DataLoader(\n",
        "    test_ds_clean,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=PIN_MEMORY,\n",
        "    collate_fn=collator\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "VKCr5hhYOkWy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256,
          "referenced_widgets": [
            "76ee334621b847c5a00c72f02c592140",
            "27deb97279b64e6fa620fb29a0612092",
            "b598d84eebc5474385c6393605c56166",
            "8a34f293210847ad82eb180613be13fc",
            "5d8365d57df1495e8f75cd7bf7c73dc2",
            "7a28428c6a5942fc9007078159e67270",
            "aa0fc39bc74c4c2bbb9a0237be904598",
            "670d9cd294d54d92b92c28ef7770fb62",
            "34959b2ff61643acb04b9400aba81963",
            "f9818c29beea4bfb8c47c1edab98b8f4",
            "b3c2f6cb3e1b4e3a8a6a602c0c7e9980",
            "9aa44bd9f13d4ea4ac03e2a9c74b34de",
            "fc82393b41b44e07bb3d32174eaf9439",
            "70c117d9990f4f129db38d0d8225fe45",
            "03d0b2edf5a5427eb79a861e57a07cc1",
            "3eba577e5a044ca2bc7deb8dff0cf7dd",
            "7ae5f4a751c248099d03f1f1c1151ed8",
            "5ca4a5dfb1a448ecae0504c0606f1ea2",
            "a6ba994f9887463a93f130314396a985",
            "f3ff21716a8d49efac24150813073337",
            "6699ab8fe8b64819bffef780afb34713",
            "fb38fbac642d4e258cbf4327a6fb204f",
            "ee9fb060ecba40c3b7f3fd7ee99c8a40",
            "1690d53a5d754b3ca4a8970f891de51c",
            "1dad298ef42043f4a75af67417f4851a",
            "dbe8d41bc1b5493b9c99940448fbde2c",
            "3418b74d305c47d5bfe63da49e98194e",
            "41192a2872d945b5ba940ad35509336a",
            "fa631f1422624295841a82b24566f513",
            "e90920f8751b4ac989844af98b47861d",
            "a3a6a9b0810c48a4a447acd37363c68e",
            "37b89f0746dc42abadbee6048820a041",
            "d9d2a863a10f40e5b91d285c262d2071",
            "5a960ace2d0d42c7a4d1043901cb446a",
            "90e8d7dc8c1b4b7bb39a583eda0bef60",
            "e58b57b389824f1c9a97d84070e6101e",
            "0d8997ed71794f99a028ef3d27c013b8",
            "fb37b18cb0b14ce0b1c75179bf3ebdc1",
            "96078218617a48189755b555bc0c5bab",
            "da4c30f5713545358ee36d23fbc87550",
            "5df14c0a1e7f45b298bf16ea0ff098f4",
            "c9263bf9854a4d38a4c94f7c81c0d6db",
            "a7f91a776a084bda86ea96a7c2d0981b",
            "ce2b967d58744c849e16d92810a28929"
          ]
        },
        "outputId": "4715c555-fb77-43ea-fa40-8bb4a25f9748"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "76ee334621b847c5a00c72f02c592140"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9aa44bd9f13d4ea4ac03e2a9c74b34de"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee9fb060ecba40c3b7f3fd7ee99c8a40"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a960ace2d0d42c7a4d1043901cb446a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ CUSTOM ROBERTA WITH DROPOUT ============\n",
        "class RobertaWithDropout(nn.Module):\n",
        "    \"\"\"\n",
        "    Custom RoBERTa model with configurable dropout\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name, num_labels, dropout_rate=0.1):\n",
        "        super(RobertaWithDropout, self).__init__()\n",
        "\n",
        "        self.roberta = AutoModel.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.classifier = nn.Linear(self.roberta.config.hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return type('obj', (object,), {'logits': logits})()\n"
      ],
      "metadata": {
        "id": "BW02_BDJPKB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel, DataCollatorWithPadding\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
        "\n",
        "# ===== config =====\n",
        "# --------- config ----------\n",
        "MODEL_NAME = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "#MODEL_DIR  = \"/content/drive/MyDrive/deep_learning\"\n",
        "MODEL_DIR  =data_dir\n",
        "\n",
        "# מודלים להערכה (רק API)\n",
        "MODEL_PREPROC = {\n",
        "    \"HF_best_model_stage3_YR.pt\": \"light\",\n",
        "}\n",
        "\n",
        "ORDERED_LABELS = ['Extremely Negative', 'Negative', 'Neutral', 'Positive', 'Extremely Positive']\n",
        "LABEL2ID = {l:i for i,l in enumerate(ORDERED_LABELS)}\n",
        "\n",
        "# ===== tokenizer + collator =====\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_CHECKPOINT, use_fast=True)\n",
        "collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# ===== preprocessing functions =====\n",
        "def preprocess_clean(t: str) -> str:\n",
        "    import re, html\n",
        "    if not isinstance(t, str): return \"\"\n",
        "    toks=[]\n",
        "    for w in t.split(\" \"):\n",
        "        if w.startswith(\"@\") and len(w)>1: toks.append(\"@user\")\n",
        "        elif w.startswith(\"http\"): toks.append(\"http\")\n",
        "        else: toks.append(w)\n",
        "    t=\" \".join(toks)\n",
        "    t=re.sub(r\"\\b(coronaviruspandemic|covid[_\\s-]*2019|covid[_\\s-]*19|covid2019|coronavirus2019|coronavirus|corona)\\b\",\n",
        "             \"covid\", t, flags=re.IGNORECASE)\n",
        "    t=html.unescape(t)\n",
        "    t=re.sub(r\"\\s+\",\" \",t).strip()\n",
        "    return t\n",
        "\n",
        "def preprocess_light(t: str) -> str:\n",
        "    import re\n",
        "    if not isinstance(t, str): return \"\"\n",
        "    t=re.sub(r'https?://\\S+','HTTPURL',t)\n",
        "    t=re.sub(r'@\\w+','@USER',t)\n",
        "    t=re.sub(r'\\s+',' ',t).strip()\n",
        "    return t\n",
        "\n",
        "# ===== dataset =====\n",
        "class TDataset(Dataset):\n",
        "    def __init__(self, texts, labels, max_len=128):\n",
        "        self.texts=texts; self.labels=labels; self.max_len=max_len\n",
        "    def __len__(self): return len(self.texts)\n",
        "    def __getitem__(self, i):\n",
        "        enc = tokenizer(self.texts[i], truncation=True, max_length=self.max_len, return_tensors=\"pt\")\n",
        "        item = {k:v.squeeze(0) for k,v in enc.items()}\n",
        "        item[\"labels\"] = torch.tensor(self.labels[i], dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "# ===== model class =====\n",
        "class RobertaWithDropout(nn.Module):\n",
        "    def __init__(self, model_name, num_labels=5, dropout_rate=0.2):\n",
        "        super().__init__()\n",
        "        self.roberta = AutoModel.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.classifier = nn.Linear(self.roberta.config.hidden_size, num_labels)\n",
        "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
        "        out = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        cls = out.last_hidden_state[:,0,:]\n",
        "        logits = self.classifier(self.dropout(cls))\n",
        "        return {\"logits\": logits}\n",
        "\n",
        "# ===== evaluation =====\n",
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            ids = batch[\"input_ids\"].to(DEVICE)\n",
        "            mask = batch.get(\"attention_mask\")\n",
        "            if mask is not None:\n",
        "                mask = mask.to(DEVICE)\n",
        "            labels = batch[\"labels\"].cpu().numpy()\n",
        "            logits = model(ids, attention_mask=mask)[\"logits\"]\n",
        "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels)\n",
        "    print(classification_report(all_labels, all_preds, target_names=ORDERED_LABELS, digits=4))\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(all_labels, all_preds),\n",
        "        \"f1\": f1_score(all_labels, all_preds, average='macro'),\n",
        "        \"precision\": precision_score(all_labels, all_preds, average='macro'),\n",
        "        \"recall\": recall_score(all_labels, all_preds, average='macro')\n",
        "    }\n",
        "\n",
        "# ===== run all API models =====\n",
        "test_df[\"label\"] = test_df[\"Sentiment\"].map(LABEL2ID)\n",
        "\n",
        "for model_file, preproc_type in MODEL_PREPROC.items():\n",
        "    print(f\"\\n=== Evaluating {model_file} ({preproc_type}) ===\")\n",
        "    if preproc_type == \"clean\":\n",
        "        texts = [preprocess_clean(x) for x in test_df[\"OriginalTweet\"].astype(str)]\n",
        "    else:\n",
        "        texts = [preprocess_light(x) for x in test_df[\"OriginalTweet\"].astype(str)]\n",
        "    labels = test_df[\"label\"].tolist()\n",
        "\n",
        "    ds = TDataset(texts, labels)\n",
        "    dl = DataLoader(ds, batch_size=128, shuffle=False, collate_fn=collator)\n",
        "\n",
        "    model = RobertaWithDropout(BASE_CHECKPOINT, num_labels=len(ORDERED_LABELS), dropout_rate=0.2)\n",
        "    model.load_state_dict(torch.load(os.path.join(MODEL_DIR, model_file), map_location=DEVICE))\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    metrics = evaluate_model(model, dl)\n",
        "    print(f\"Acc={metrics['accuracy']:.4f}, F1={metrics['f1']:.4f}, \"\n",
        "          f\"Precision={metrics['precision']:.4f}, Recall={metrics['recall']:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358,
          "referenced_widgets": [
            "8e6243b94890465c940a1da30a9b26b6",
            "6d5585ff278541b4968f8b346ba7c2d5",
            "022f704fa8444eb7abec242c44f3b816",
            "b907967cb0c74542bac4b7a4c157d444",
            "2b28c66b3daa4337adff34d4836166ae",
            "3ad3328082fd4226acc0a470c3ce5feb",
            "0a51cd802ca048fd9ecd486aeb7f5ffc",
            "e212984111f3420a92845f7231561c53",
            "71c35030b6db4890a649ece5b6ffa902",
            "d098613fdc894d89960e3cfbd2080a18",
            "c52ed5e048284f98a7088539575a5545",
            "00e508b34d784c2083020095643af3ba",
            "2c52c92f8d3b4dbbab696a147614d5cc",
            "5969ac09803b4a4691836eeca02bc910",
            "0a21e481f5824d7da7b7ffd6581d2002",
            "79559d31e1214c9085ff5b96333569ee",
            "4b9a339706c64e78a26dbc160869b150",
            "fb1bcec808ba450a818c88e0bedc014a",
            "1100d8d5db9d4cf9a5a022947e9e4d19",
            "8b0021002aa3447e88db198840667690",
            "1af77345a4fa41a8af4ed1693d589d5e",
            "a062d61d2b71462c9813a039420d3fff"
          ]
        },
        "id": "G6J3rvUYQ1d4",
        "outputId": "4fe51bdd-a8bb-443c-b9a6-6d01ca151700"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Evaluating HF_best_model_stage3_YR.pt (light) ===\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8e6243b94890465c940a1da30a9b26b6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/501M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "00e508b34d784c2083020095643af3ba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                    precision    recall  f1-score   support\n",
            "\n",
            "Extremely Negative     0.8929    0.8733    0.8830       592\n",
            "          Negative     0.8368    0.8569    0.8467      1041\n",
            "           Neutral     0.9011    0.8239    0.8608       619\n",
            "          Positive     0.8240    0.8353    0.8296       947\n",
            "Extremely Positive     0.8501    0.8898    0.8695       599\n",
            "\n",
            "          accuracy                         0.8539      3798\n",
            "         macro avg     0.8610    0.8558    0.8579      3798\n",
            "      weighted avg     0.8549    0.8539    0.8540      3798\n",
            "\n",
            "Acc=0.8539, F1=0.8579, Precision=0.8610, Recall=0.8558\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Post-training Dynamic Quantization"
      ],
      "metadata": {
        "id": "n81sSFuhfK6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cpu\")\n",
        "# Check\n",
        "print(f\"Using device: {DEVICE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zsbj1Z7JiGyd",
        "outputId": "752d08f0-13dc-4dec-ae80-91b41a748247"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# Step 1: Robust loader for  Roberta\n",
        "# ======================\n",
        "\n",
        "import os\n",
        "import copy\n",
        "import tempfile\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.quantization import quantize_dynamic\n",
        "\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
        "\n",
        "# ======================\n",
        "# Basic configuration\n",
        "# ======================\n",
        "MODEL_DIR  =data_dir\n",
        "MODEL_FILE  = \"HF_best_model_stage3_YR.pt\"\n",
        "BACKBONE    = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "NUM_LABELS  = 5\n",
        "DROPOUT     = 0.2\n",
        "OUT_DIR     = os.path.join(MODEL_DIR, \"quantized_int8\")\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "ordered_labels = [\"Extremely Negative\",\"Negative\",\"Neutral\",\"Positive\",\"Extremely Positive\"]\n",
        "\n",
        "def _state_dict_size_mb(model_or_sd) -> float:\n",
        "    sd = model_or_sd if isinstance(model_or_sd, dict) else model_or_sd.state_dict()\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".pt\") as tmp:\n",
        "        torch.save(sd, tmp.name)\n",
        "        size_mb = os.path.getsize(tmp.name) / (1024**2)\n",
        "    os.remove(tmp.name)\n",
        "    return size_mb\n",
        "\n",
        "# ======================\n",
        "# 1) Load FP32 model\n",
        "# ======================\n",
        "model_path = os.path.join(MODEL_DIR, MODEL_FILE)\n",
        "model = RobertaWithDropout(BACKBONE, num_labels=NUM_LABELS, dropout_rate=DROPOUT)\n",
        "state_dict = torch.load(model_path, map_location=\"cpu\")\n",
        "model.load_state_dict(state_dict, strict=True)\n",
        "model.eval().cpu()\n",
        "\n",
        "param_count = sum(p.numel() for p in model.parameters())\n",
        "size_fp32   = _state_dict_size_mb(model)\n",
        "print(f\"Loaded {MODEL_FILE} | params={param_count:,} | state_dict={size_fp32:.2f} MB\")\n",
        "\n",
        "# ======================\n",
        "# 2) Dynamic quantization on encoder only (INT8)\n",
        "# ======================\n",
        "q_model = copy.deepcopy(model).eval().cpu()\n",
        "assert hasattr(q_model, \"roberta\") and hasattr(q_model.roberta, \"encoder\"), \"Expected .roberta.encoder to exist\"\n",
        "q_model.roberta.encoder = quantize_dynamic(q_model.roberta.encoder, {nn.Linear}, dtype=torch.qint8)\n",
        "\n",
        "size_int8 = _state_dict_size_mb(q_model)\n",
        "print(f\"State dict size: FP32={size_fp32:.2f} MB -> INT8={size_int8:.2f} MB\")\n",
        "\n",
        "# ======================\n",
        "# 3) Save quantized state_dict\n",
        "# ======================\n",
        "save_name  = f\"{Path(MODEL_FILE).stem}_quantized_int8_sd.pt\"\n",
        "quant_path = os.path.join(OUT_DIR, save_name)\n",
        "torch.save(q_model.state_dict(), quant_path)\n",
        "print(f\"Quantized model saved to: {quant_path}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBGv1XPOfRKw",
        "outputId": "7c28f286-6582-4398-ee08-018817bf8aab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded HF_best_model_stage3_YR.pt | params=124,649,477 | state_dict=475.58 MB\n",
            "State dict size: FP32=475.58 MB -> INT8=232.63 MB\n",
            "Quantized model saved to: /content/drive/MyDrive/deep_learning/quantized_int8/HF_best_model_stage3_YR_quantized_int8_sd.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# 4) Optional evaluation on CPU (runs only if `test_df` exists)\n",
        "# ======================\n",
        "@torch.inference_mode()\n",
        "def eval_int8_cpu(model, dataloader, label_names=None):\n",
        "    \"\"\"Evaluate model on CPU and print a per-class report.\"\"\"\n",
        "    model.eval()\n",
        "    model.cpu()\n",
        "    y_true, y_pred = [], []\n",
        "\n",
        "    for batch in dataloader:\n",
        "        labels = batch.pop(\"labels\")\n",
        "        # Ensure tensors\n",
        "        batch = {k: (v if isinstance(v, torch.Tensor) else torch.tensor(v)) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "\n",
        "        # Robust logits extraction\n",
        "        if hasattr(outputs, \"logits\"):\n",
        "            logits = outputs.logits\n",
        "        elif isinstance(outputs, dict) and \"logits\" in outputs:\n",
        "            logits = outputs[\"logits\"]\n",
        "        elif isinstance(outputs, (list, tuple)):\n",
        "            logits = outputs[0]\n",
        "        else:\n",
        "            logits = outputs  # if the model returns logits directly\n",
        "\n",
        "        preds = torch.argmax(logits, dim=-1)\n",
        "        y_true.extend(labels.cpu().tolist())\n",
        "        y_pred.extend(preds.cpu().tolist())\n",
        "\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_true, y_pred, target_names=label_names, digits=4))\n",
        "\n",
        "    metrics = {\n",
        "        \"accuracy\":  accuracy_score(y_true, y_pred),\n",
        "        \"f1\":        f1_score(y_true, y_pred, average=\"macro\"),\n",
        "        \"precision\": precision_score(y_true, y_pred, average=\"macro\", zero_division=0),\n",
        "        \"recall\":    recall_score(y_true, y_pred, average=\"macro\"),\n",
        "    }\n",
        "    print(\n",
        "        f\"Accuracy={metrics['accuracy']:.4f} | F1(macro)={metrics['f1']:.4f} | \"\n",
        "        f\"Precision(macro)={metrics['precision']:.4f} | Recall(macro)={metrics['recall']:.4f}\"\n",
        "    )\n",
        "    return metrics\n",
        "\n",
        "\n",
        "class _EvalRobertaDS(Dataset):\n",
        "    \"\"\"Simple dataset for preprocessed texts and integer labels.\"\"\"\n",
        "    def __init__(self, texts, labels, max_len=128, backbone=BACKBONE):\n",
        "        self.texts = list(texts)\n",
        "        self.labels = list(labels)\n",
        "        self.max_len = max_len\n",
        "        self.tok = AutoTokenizer.from_pretrained(backbone, use_fast=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        enc = self.tok(self.texts[i], truncation=True, max_length=self.max_len)\n",
        "        enc[\"labels\"] = int(self.labels[i])\n",
        "        return enc\n",
        "\n",
        "# === 4) Evaluate with your existing loader ===\n",
        "print(\"Evaluating INT8 with existing test_loader_clean...\")\n",
        "int8_metrics = eval_int8_cpu(q_model, test_loader_clean, label_names=ordered_labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMUIlyP-fjUv",
        "outputId": "22aedf25-32d3-41a3-cba4-7997b3beeace"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating INT8 with existing test_loader_clean...\n",
            "Classification Report:\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            "Extremely Negative     0.8943    0.8429    0.8678       592\n",
            "          Negative     0.8023    0.8732    0.8362      1041\n",
            "           Neutral     0.8946    0.8498    0.8716       619\n",
            "          Positive     0.8128    0.7930    0.8028       947\n",
            "Extremely Positive     0.8538    0.8481    0.8509       599\n",
            "\n",
            "          accuracy                         0.8407      3798\n",
            "         macro avg     0.8515    0.8414    0.8459      3798\n",
            "      weighted avg     0.8424    0.8407    0.8409      3798\n",
            "\n",
            "Accuracy=0.8407 | F1(macro)=0.8459 | Precision(macro)=0.8515 | Recall(macro)=0.8414\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# unstructured pruning"
      ],
      "metadata": {
        "id": "A9Je1Zdznw50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prune_best_model2_roberta.py\n",
        "import os\n",
        "import tempfile\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils import prune\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "# If RobertaWithDropout is defined elsewhere, import it:\n",
        "# from your_model_defs import RobertaWithDropout\n",
        "\n",
        "# ======================\n",
        "# Basic configuration\n",
        "# ======================\n",
        "MODEL_DIR  =data_dir\n",
        "MODEL_FILE = \"HF_best_model_stage3_YR.pt\"\n",
        "BACKBONE   = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "NUM_LABELS = 5\n",
        "DROPOUT    = 0.2\n",
        "\n",
        "PRUNE_AMOUNT         = 0.40  # 40% global unstructured L1\n",
        "PRUNE_CLASSIFIER     = False # keep classifier dense by default\n",
        "PRUNED_SD_SAVE_PATH  = os.path.join(MODEL_DIR, f\"{Path(MODEL_FILE).stem}_pruned40_sd.pt\")\n",
        "\n",
        "# Use your existing labels or override here\n",
        "ordered_labels = [\"Extremely Negative\",\"Negative\",\"Neutral\",\"Positive\",\"Extremely Positive\"]\n",
        "\n",
        "# ======================\n",
        "# Utilities\n",
        "# ======================\n",
        "def state_dict_size_mb(model_or_sd) -> float:\n",
        "    sd = model_or_sd if isinstance(model_or_sd, dict) else model_or_sd.state_dict()\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".pt\") as tmp:\n",
        "        torch.save(sd, tmp.name)\n",
        "        size_mb = os.path.getsize(tmp.name) / (1024**2)\n",
        "    os.remove(tmp.name)\n",
        "    return size_mb\n",
        "\n",
        "def load_fp32_model_state(model_dir, model_file):\n",
        "    \"\"\"Load FP32 weights into the exact architecture wrapper.\"\"\"\n",
        "    path = os.path.join(model_dir, model_file)\n",
        "    model = RobertaWithDropout(BACKBONE, num_labels=NUM_LABELS, dropout_rate=DROPOUT)\n",
        "    state = torch.load(path, map_location=\"cpu\")\n",
        "    model.load_state_dict(state, strict=True)\n",
        "    model.eval().cpu()\n",
        "    return model\n",
        "\n",
        "def collect_encoder_linear_params(model, prune_classifier=False):\n",
        "    \"\"\"\n",
        "    Collect (module, 'weight') pairs for nn.Linear layers.\n",
        "    By default, only prune Linear layers under 'roberta.encoder'.\n",
        "    If prune_classifier=True, also include classifier Linear layers.\n",
        "    \"\"\"\n",
        "    targets = []\n",
        "    for name, module in model.named_modules():\n",
        "        if not isinstance(module, nn.Linear):\n",
        "            continue\n",
        "        is_encoder = name.startswith(\"roberta.encoder\")\n",
        "        is_classifier = name.startswith(\"classifier\") or \".classifier.\" in name\n",
        "        if is_encoder:\n",
        "            targets.append((module, \"weight\"))\n",
        "        elif prune_classifier and is_classifier:\n",
        "            targets.append((module, \"weight\"))\n",
        "    return targets\n",
        "\n",
        "def apply_global_l1(model, amount=0.4, prune_classifier=False):\n",
        "    \"\"\"Apply global unstructured L1 pruning over selected Linear weights.\"\"\"\n",
        "    targets = collect_encoder_linear_params(model, prune_classifier=prune_classifier)\n",
        "    if len(targets) == 0:\n",
        "        raise RuntimeError(\"No Linear layers collected for pruning. Check your module naming/paths.\")\n",
        "    prune.global_unstructured(targets, pruning_method=prune.L1Unstructured, amount=amount)\n",
        "\n",
        "    # Report density/sparsity on masked weights (viewed with the pruning mask)\n",
        "    total, nonzero = 0, 0\n",
        "    for m, _ in targets:\n",
        "        w = m.weight  # masked view\n",
        "        total   += w.numel()\n",
        "        nonzero += (w != 0).sum().item()\n",
        "    density  = nonzero / total if total else 1.0\n",
        "    sparsity = 1.0 - density\n",
        "    print(f\"Applied global L1 pruning: amount={amount:.2f} | density={density:.2%} | sparsity={sparsity:.2%}\")\n",
        "    return targets, density, sparsity\n",
        "\n",
        "def remove_pruning_masks(targets):\n",
        "    \"\"\"Make pruning permanent by removing mask reparameterizations.\"\"\"\n",
        "    for m, _ in targets:\n",
        "        prune.remove(m, \"weight\")\n",
        "\n",
        "@torch.inference_mode()\n",
        "def evaluate_model_cpu(model, dataloader, label_names=None):\n",
        "    \"\"\"Generic CPU evaluation for classification models that return logits.\"\"\"\n",
        "    model.eval().cpu()\n",
        "    y_true, y_pred = [], []\n",
        "    for batch in dataloader:\n",
        "        # Ensure tensors are on CPU\n",
        "        for k, v in list(batch.items()):\n",
        "            if isinstance(v, torch.Tensor):\n",
        "                batch[k] = v.cpu()\n",
        "\n",
        "        labels = batch.pop(\"labels\") if \"labels\" in batch else batch.pop(\"label\", None)\n",
        "        outputs = model(**batch)\n",
        "\n",
        "        # Robust logits extraction\n",
        "        if hasattr(outputs, \"logits\"):\n",
        "            logits = outputs.logits\n",
        "        elif isinstance(outputs, dict) and \"logits\" in outputs:\n",
        "            logits = outputs[\"logits\"]\n",
        "        elif isinstance(outputs, (list, tuple)):\n",
        "            logits = outputs[0]\n",
        "        else:\n",
        "            logits = outputs\n",
        "\n",
        "        preds = torch.argmax(logits, dim=-1)\n",
        "        y_pred.extend(preds.cpu().tolist())\n",
        "        if labels is not None:\n",
        "            y_true.extend(labels.cpu().tolist())\n",
        "\n",
        "    if len(y_true) > 0:\n",
        "        print(\"Classification Report:\")\n",
        "        print(classification_report(y_true, y_pred, target_names=label_names, digits=4))\n",
        "        metrics = {\n",
        "            \"accuracy\":  accuracy_score(y_true, y_pred),\n",
        "            \"f1\":        f1_score(y_true, y_pred, average=\"macro\"),\n",
        "            \"precision\": precision_score(y_true, y_pred, average=\"macro\", zero_division=0),\n",
        "            \"recall\":    recall_score(y_true, y_pred, average=\"macro\"),\n",
        "        }\n",
        "        print(\n",
        "            f\"Accuracy={metrics['accuracy']:.4f} | F1(macro)={metrics['f1']:.4f} | \"\n",
        "            f\"Precision(macro)={metrics['precision']:.4f} | Recall(macro)={metrics['recall']:.4f}\"\n",
        "        )\n",
        "        return metrics\n",
        "    else:\n",
        "        print(\"Labels not found in dataloader batches; returned predictions only.\")\n",
        "        return {}\n",
        "\n",
        "# ======================\n",
        "# Run pruning workflow\n",
        "# ======================\n",
        "model = load_fp32_model_state(MODEL_DIR, MODEL_FILE)\n",
        "print(f\"Baseline state_dict size (dense): {state_dict_size_mb(model):.2f} MB\")\n",
        "\n",
        "targets, density, sparsity = apply_global_l1(\n",
        "    model,\n",
        "    amount=PRUNE_AMOUNT,\n",
        "    prune_classifier=PRUNE_CLASSIFIER\n",
        ")\n",
        "\n",
        "# (Optional) short fine-tuning step to recover accuracy could be placed here.\n",
        "\n",
        "remove_pruning_masks(targets)\n",
        "print(f\"Pruned state_dict size (dense save): {state_dict_size_mb(model):.2f} MB\")\n",
        "\n",
        "torch.save(model.state_dict(), PRUNED_SD_SAVE_PATH)\n",
        "print(\"Saved pruned state_dict to:\", PRUNED_SD_SAVE_PATH)\n",
        "\n",
        "# ======================\n",
        "# Evaluate with your existing loader\n",
        "# ======================\n",
        "# Assumes you already have: test_loader_clean\n",
        "# If your loader is named differently, replace below.\n",
        "if \"test_loader_clean\" in globals():\n",
        "    print(\"Evaluating PRUNED model on test_loader_clean (CPU)...\")\n",
        "    pruned_metrics = evaluate_model_cpu(model, test_loader_clean, label_names=ordered_labels)\n",
        "else:\n",
        "    print(\"Skipping evaluation: `test_loader_clean` not found in globals().\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QU56yirjm2H",
        "outputId": "34869bfa-d212-4545-dc93-91d23931e7aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline state_dict size (dense): 475.58 MB\n",
            "Applied global L1 pruning: amount=0.40 | density=60.00% | sparsity=40.00%\n",
            "Pruned state_dict size (dense save): 475.58 MB\n",
            "Saved pruned state_dict to: /content/drive/MyDrive/deep_learning/HF_best_model_stage3_YR_pruned40_sd.pt\n",
            "Evaluating PRUNED model on test_loader_clean (CPU)...\n",
            "Classification Report:\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            "Extremely Negative     0.9875    0.1334    0.2351       592\n",
            "          Negative     0.6047    0.8742    0.7148      1041\n",
            "           Neutral     0.8526    0.8691    0.8608       619\n",
            "          Positive     0.5912    0.8933    0.7115       947\n",
            "Extremely Positive     0.9868    0.2487    0.3973       599\n",
            "\n",
            "          accuracy                         0.6640      3798\n",
            "         macro avg     0.8045    0.6038    0.5839      3798\n",
            "      weighted avg     0.7616    0.6640    0.6130      3798\n",
            "\n",
            "Accuracy=0.6640 | F1(macro)=0.5839 | Precision(macro)=0.8045 | Recall(macro)=0.6038\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prune_best_model2_roberta.py\n",
        "import os\n",
        "import tempfile\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils import prune\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "# If RobertaWithDropout is defined elsewhere, import it:\n",
        "# from your_model_defs import RobertaWithDropout\n",
        "\n",
        "# ======================\n",
        "# Basic configuration\n",
        "# ======================\n",
        "MODEL_DIR  =data_dir\n",
        "MODEL_FILE = \"HF_best_model_stage3_YR.pt\"\n",
        "BACKBONE   = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "NUM_LABELS = 5\n",
        "DROPOUT    = 0.2\n",
        "\n",
        "PRUNE_AMOUNT         = 0.30  # 30% global unstructured L1\n",
        "PRUNE_CLASSIFIER     = False # keep classifier dense by default\n",
        "PRUNED_SD_SAVE_PATH  = os.path.join(MODEL_DIR, f\"{Path(MODEL_FILE).stem}_pruned30_sd.pt\")\n",
        "\n",
        "# Use your existing labels or override here\n",
        "ordered_labels = [\"Extremely Negative\",\"Negative\",\"Neutral\",\"Positive\",\"Extremely Positive\"]\n",
        "\n",
        "# ======================\n",
        "# Utilities\n",
        "# ======================\n",
        "def state_dict_size_mb(model_or_sd) -> float:\n",
        "    sd = model_or_sd if isinstance(model_or_sd, dict) else model_or_sd.state_dict()\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".pt\") as tmp:\n",
        "        torch.save(sd, tmp.name)\n",
        "        size_mb = os.path.getsize(tmp.name) / (1024**2)\n",
        "    os.remove(tmp.name)\n",
        "    return size_mb\n",
        "\n",
        "def load_fp32_model_state(model_dir, model_file):\n",
        "    \"\"\"Load FP32 weights into the exact architecture wrapper.\"\"\"\n",
        "    path = os.path.join(model_dir, model_file)\n",
        "    model = RobertaWithDropout(BACKBONE, num_labels=NUM_LABELS, dropout_rate=DROPOUT)\n",
        "    state = torch.load(path, map_location=\"cpu\")\n",
        "    model.load_state_dict(state, strict=True)\n",
        "    model.eval().cpu()\n",
        "    return model\n",
        "\n",
        "def collect_encoder_linear_params(model, prune_classifier=False):\n",
        "    \"\"\"\n",
        "    Collect (module, 'weight') pairs for nn.Linear layers.\n",
        "    By default, only prune Linear layers under 'roberta.encoder'.\n",
        "    If prune_classifier=True, also include classifier Linear layers.\n",
        "    \"\"\"\n",
        "    targets = []\n",
        "    for name, module in model.named_modules():\n",
        "        if not isinstance(module, nn.Linear):\n",
        "            continue\n",
        "        is_encoder = name.startswith(\"roberta.encoder\")\n",
        "        is_classifier = name.startswith(\"classifier\") or \".classifier.\" in name\n",
        "        if is_encoder:\n",
        "            targets.append((module, \"weight\"))\n",
        "        elif prune_classifier and is_classifier:\n",
        "            targets.append((module, \"weight\"))\n",
        "    return targets\n",
        "\n",
        "def apply_global_l1(model, amount=0.4, prune_classifier=False):\n",
        "    \"\"\"Apply global unstructured L1 pruning over selected Linear weights.\"\"\"\n",
        "    targets = collect_encoder_linear_params(model, prune_classifier=prune_classifier)\n",
        "    if len(targets) == 0:\n",
        "        raise RuntimeError(\"No Linear layers collected for pruning. Check your module naming/paths.\")\n",
        "    prune.global_unstructured(targets, pruning_method=prune.L1Unstructured, amount=amount)\n",
        "\n",
        "    # Report density/sparsity on masked weights (viewed with the pruning mask)\n",
        "    total, nonzero = 0, 0\n",
        "    for m, _ in targets:\n",
        "        w = m.weight  # masked view\n",
        "        total   += w.numel()\n",
        "        nonzero += (w != 0).sum().item()\n",
        "    density  = nonzero / total if total else 1.0\n",
        "    sparsity = 1.0 - density\n",
        "    print(f\"Applied global L1 pruning: amount={amount:.2f} | density={density:.2%} | sparsity={sparsity:.2%}\")\n",
        "    return targets, density, sparsity\n",
        "\n",
        "def remove_pruning_masks(targets):\n",
        "    \"\"\"Make pruning permanent by removing mask reparameterizations.\"\"\"\n",
        "    for m, _ in targets:\n",
        "        prune.remove(m, \"weight\")\n",
        "\n",
        "@torch.inference_mode()\n",
        "def evaluate_model_cpu(model, dataloader, label_names=None):\n",
        "    \"\"\"Generic CPU evaluation for classification models that return logits.\"\"\"\n",
        "    model.eval().cpu()\n",
        "    y_true, y_pred = [], []\n",
        "    for batch in dataloader:\n",
        "        # Ensure tensors are on CPU\n",
        "        for k, v in list(batch.items()):\n",
        "            if isinstance(v, torch.Tensor):\n",
        "                batch[k] = v.cpu()\n",
        "\n",
        "        labels = batch.pop(\"labels\") if \"labels\" in batch else batch.pop(\"label\", None)\n",
        "        outputs = model(**batch)\n",
        "\n",
        "        # Robust logits extraction\n",
        "        if hasattr(outputs, \"logits\"):\n",
        "            logits = outputs.logits\n",
        "        elif isinstance(outputs, dict) and \"logits\" in outputs:\n",
        "            logits = outputs[\"logits\"]\n",
        "        elif isinstance(outputs, (list, tuple)):\n",
        "            logits = outputs[0]\n",
        "        else:\n",
        "            logits = outputs\n",
        "\n",
        "        preds = torch.argmax(logits, dim=-1)\n",
        "        y_pred.extend(preds.cpu().tolist())\n",
        "        if labels is not None:\n",
        "            y_true.extend(labels.cpu().tolist())\n",
        "\n",
        "    if len(y_true) > 0:\n",
        "        print(\"Classification Report:\")\n",
        "        print(classification_report(y_true, y_pred, target_names=label_names, digits=4))\n",
        "        metrics = {\n",
        "            \"accuracy\":  accuracy_score(y_true, y_pred),\n",
        "            \"f1\":        f1_score(y_true, y_pred, average=\"macro\"),\n",
        "            \"precision\": precision_score(y_true, y_pred, average=\"macro\", zero_division=0),\n",
        "            \"recall\":    recall_score(y_true, y_pred, average=\"macro\"),\n",
        "        }\n",
        "        print(\n",
        "            f\"Accuracy={metrics['accuracy']:.4f} | F1(macro)={metrics['f1']:.4f} | \"\n",
        "            f\"Precision(macro)={metrics['precision']:.4f} | Recall(macro)={metrics['recall']:.4f}\"\n",
        "        )\n",
        "        return metrics\n",
        "    else:\n",
        "        print(\"Labels not found in dataloader batches; returned predictions only.\")\n",
        "        return {}\n",
        "\n",
        "# ======================\n",
        "# Run pruning workflow\n",
        "# ======================\n",
        "model = load_fp32_model_state(MODEL_DIR, MODEL_FILE)\n",
        "print(f\"Baseline state_dict size (dense): {state_dict_size_mb(model):.2f} MB\")\n",
        "\n",
        "targets, density, sparsity = apply_global_l1(\n",
        "    model,\n",
        "    amount=PRUNE_AMOUNT,\n",
        "    prune_classifier=PRUNE_CLASSIFIER\n",
        ")\n",
        "\n",
        "# (Optional) short fine-tuning step to recover accuracy could be placed here.\n",
        "\n",
        "remove_pruning_masks(targets)\n",
        "print(f\"Pruned state_dict size (dense save): {state_dict_size_mb(model):.2f} MB\")\n",
        "\n",
        "torch.save(model.state_dict(), PRUNED_SD_SAVE_PATH)\n",
        "print(\"Saved pruned state_dict to:\", PRUNED_SD_SAVE_PATH)\n",
        "\n",
        "# ======================\n",
        "# Evaluate with your existing loader\n",
        "# ======================\n",
        "# Assumes you already have: test_loader_clean\n",
        "# If your loader is named differently, replace below.\n",
        "if \"test_loader_clean\" in globals():\n",
        "    print(\"Evaluating PRUNED model on test_loader_clean (CPU)...\")\n",
        "    pruned_metrics = evaluate_model_cpu(model, test_loader_clean, label_names=ordered_labels)\n",
        "else:\n",
        "    print(\"Skipping evaluation: `test_loader_clean` not found in globals().\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tMfar3PqRr7",
        "outputId": "ab0d4ecb-e9bd-4614-f40f-47aa57813e24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline state_dict size (dense): 475.58 MB\n",
            "Applied global L1 pruning: amount=0.30 | density=70.00% | sparsity=30.00%\n",
            "Pruned state_dict size (dense save): 475.58 MB\n",
            "Saved pruned state_dict to: /content/drive/MyDrive/deep_learning/HF_best_model_stage3_YR_pruned30_sd.pt\n",
            "Evaluating PRUNED model on test_loader_clean (CPU)...\n",
            "Classification Report:\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            "Extremely Negative     0.9723    0.5338    0.6892       592\n",
            "          Negative     0.7106    0.9059    0.7965      1041\n",
            "           Neutral     0.8864    0.8449    0.8652       619\n",
            "          Positive     0.7054    0.8849    0.7850       947\n",
            "Extremely Positive     0.9538    0.5860    0.7260       599\n",
            "\n",
            "          accuracy                         0.7823      3798\n",
            "         macro avg     0.8457    0.7511    0.7724      3798\n",
            "      weighted avg     0.8171    0.7823    0.7770      3798\n",
            "\n",
            "Accuracy=0.7823 | F1(macro)=0.7724 | Precision(macro)=0.8457 | Recall(macro)=0.7511\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# replace collect_encoder_linear_params(...) with:\n",
        "def collect_ffn_linear_params_only(model, prune_classifier=False):\n",
        "    \"\"\"\n",
        "    Collect (name, module) for FFN Linear layers only:\n",
        "    - ...intermediate.dense\n",
        "    - ...output.dense\n",
        "    Leaves attention (q/k/v/out_proj) untouched.\n",
        "    \"\"\"\n",
        "    pairs = []\n",
        "    for name, module in model.named_modules():\n",
        "        if not isinstance(module, nn.Linear):\n",
        "            continue\n",
        "        if name.startswith(\"roberta.encoder\"):\n",
        "            is_ffn = (\".intermediate.dense\" in name) or (\".output.dense\" in name)\n",
        "            if is_ffn:\n",
        "                pairs.append((name, module))\n",
        "        elif prune_classifier and (\"classifier\" in name):\n",
        "            pairs.append((name, module))\n",
        "    return pairs\n",
        "\n",
        "  # new: uniform per-layer L1 pruning on FFN only\n",
        "def apply_uniform_l1_ffn(model, amount_per_layer=0.22, prune_classifier=False):\n",
        "    pairs = collect_ffn_linear_params_only(model, prune_classifier=prune_classifier)\n",
        "    if len(pairs) == 0:\n",
        "        raise RuntimeError(\"No FFN Linear layers collected for pruning.\")\n",
        "\n",
        "    # apply same amount per layer\n",
        "    for _, module in pairs:\n",
        "        prune.l1_unstructured(module, name=\"weight\", amount=amount_per_layer)\n",
        "\n",
        "    # report density/sparsity\n",
        "    total, nonzero = 0, 0\n",
        "    for _, m in pairs:\n",
        "        w = m.weight\n",
        "        total   += w.numel()\n",
        "        nonzero += (w != 0).sum().item()\n",
        "    density  = nonzero / total if total else 1.0\n",
        "    sparsity = 1.0 - density\n",
        "    print(f\"Applied FFN-only uniform L1: per-layer amount={amount_per_layer:.2f} | \"\n",
        "          f\"density={density:.2%} | sparsity={sparsity:.2%}\")\n",
        "\n",
        "    # return targets in the (module,\"weight\") shape for remove()\n",
        "    targets = [(m, \"weight\") for _, m in pairs]\n",
        "    return targets, density, sparsity\n",
        "\n"
      ],
      "metadata": {
        "id": "I1aniMoarRIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prune_hf_stage3_YR_ffn_uniform.py\n",
        "import os\n",
        "import tempfile\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils import prune\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "# from your_model_defs import RobertaWithDropout\n",
        "\n",
        "# ======================\n",
        "# Basic configuration\n",
        "# ======================\n",
        "MODEL_DIR  =data_dir\n",
        "MODEL_FILE = \"HF_best_model_stage3_YR.pt\"\n",
        "BACKBONE   = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "NUM_LABELS = 5\n",
        "DROPOUT    = 0.2\n",
        "\n",
        "PRUNE_PER_LAYER      = 0.22   # try 0.20–0.25; tune to reach ≥80%\n",
        "PRUNE_CLASSIFIER     = False  # keep classifier dense by default\n",
        "PRUNED_SD_SAVE_PATH  = os.path.join(\n",
        "    MODEL_DIR, f\"{Path(MODEL_FILE).stem}_ffn_uniform{int(PRUNE_PER_LAYER*100)}_sd.pt\"\n",
        ")\n",
        "\n",
        "ordered_labels = [\"Extremely Negative\",\"Negative\",\"Neutral\",\"Positive\",\"Extremely Positive\"]\n",
        "\n",
        "# ======================\n",
        "# Utilities\n",
        "# ======================\n",
        "def state_dict_size_mb(model_or_sd) -> float:\n",
        "    sd = model_or_sd if isinstance(model_or_sd, dict) else model_or_sd.state_dict()\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".pt\") as tmp:\n",
        "        torch.save(sd, tmp.name)\n",
        "        size_mb = os.path.getsize(tmp.name) / (1024**2)\n",
        "    os.remove(tmp.name)\n",
        "    return size_mb\n",
        "\n",
        "def load_fp32_model_state(model_dir, model_file):\n",
        "    \"\"\"Load FP32 weights into the exact architecture wrapper.\"\"\"\n",
        "    path = os.path.join(model_dir, model_file)\n",
        "    model = RobertaWithDropout(BACKBONE, num_labels=NUM_LABELS, dropout_rate=DROPOUT)\n",
        "    state = torch.load(path, map_location=\"cpu\")\n",
        "    model.load_state_dict(state, strict=True)\n",
        "    model.eval().cpu()\n",
        "    return model\n",
        "\n",
        "def collect_ffn_linear_params_only(model, prune_classifier=False):\n",
        "    \"\"\"\n",
        "    Collect Linear weights ONLY from FFN blocks under roberta.encoder:\n",
        "    - ...intermediate.dense\n",
        "    - ...output.dense\n",
        "    Leave attention (q/k/v/out_proj) untouched.\n",
        "    \"\"\"\n",
        "    pairs = []\n",
        "    for name, module in model.named_modules():\n",
        "        if not isinstance(module, nn.Linear):\n",
        "            continue\n",
        "        if name.startswith(\"roberta.encoder\"):\n",
        "            is_ffn = (\".intermediate.dense\" in name) or (\".output.dense\" in name)\n",
        "            if is_ffn:\n",
        "                pairs.append((name, module))\n",
        "        elif prune_classifier and (\"classifier\" in name):\n",
        "            pairs.append((name, module))\n",
        "    return pairs\n",
        "\n",
        "def apply_uniform_l1_ffn(model, amount_per_layer=0.22, prune_classifier=False):\n",
        "    \"\"\"Apply the SAME L1-unstructured amount per FFN Linear layer.\"\"\"\n",
        "    pairs = collect_ffn_linear_params_only(model, prune_classifier=prune_classifier)\n",
        "    if len(pairs) == 0:\n",
        "        raise RuntimeError(\"No FFN Linear layers collected for pruning.\")\n",
        "\n",
        "    for _, module in pairs:\n",
        "        prune.l1_unstructured(module, name=\"weight\", amount=amount_per_layer)\n",
        "\n",
        "    # report overall density/sparsity on masked weights\n",
        "    total, nonzero = 0, 0\n",
        "    for _, m in pairs:\n",
        "        w = m.weight\n",
        "        total   += w.numel()\n",
        "        nonzero += (w != 0).sum().item()\n",
        "    density  = nonzero / total if total else 1.0\n",
        "    sparsity = 1.0 - density\n",
        "    print(f\"Applied FFN-only uniform L1: per-layer amount={amount_per_layer:.2f} | \"\n",
        "          f\"density={density:.2%} | sparsity={sparsity:.2%}\")\n",
        "\n",
        "    # return targets in (module, \"weight\") shape so we can remove() later\n",
        "    targets = [(m, \"weight\") for _, m in pairs]\n",
        "    return targets, density, sparsity\n",
        "\n",
        "def remove_pruning_masks(targets):\n",
        "    \"\"\"Make pruning permanent by removing mask reparameterizations.\"\"\"\n",
        "    for m, _ in targets:\n",
        "        prune.remove(m, \"weight\")\n",
        "\n",
        "@torch.inference_mode()\n",
        "def evaluate_model_cpu(model, dataloader, label_names=None):\n",
        "    \"\"\"Generic CPU evaluation for classification models that return logits.\"\"\"\n",
        "    model.eval().cpu()\n",
        "    y_true, y_pred = [], []\n",
        "    for batch in dataloader:\n",
        "        # ensure tensors on CPU\n",
        "        for k, v in list(batch.items()):\n",
        "            if isinstance(v, torch.Tensor):\n",
        "                batch[k] = v.cpu()\n",
        "\n",
        "        labels = batch.pop(\"labels\") if \"labels\" in batch else batch.pop(\"label\", None)\n",
        "        batch.pop(\"token_type_ids\", None)  # RoBERTa doesn't use it\n",
        "\n",
        "        outputs = model(**batch)\n",
        "        logits = (outputs.logits if hasattr(outputs, \"logits\")\n",
        "                  else outputs[\"logits\"] if isinstance(outputs, dict)\n",
        "                  else outputs[0])\n",
        "        preds = torch.argmax(logits, dim=-1)\n",
        "        y_pred.extend(preds.cpu().tolist())\n",
        "        if labels is not None:\n",
        "            y_true.extend(labels.cpu().tolist())\n",
        "\n",
        "    if len(y_true) > 0:\n",
        "        print(\"\\n=== Classification Report ===\")\n",
        "        print(classification_report(y_true, y_pred, target_names=label_names, digits=4))\n",
        "        metrics = {\n",
        "            \"accuracy\":  accuracy_score(y_true, y_pred),\n",
        "            \"f1\":        f1_score(y_true, y_pred, average=\"macro\"),\n",
        "            \"precision\": precision_score(y_true, y_pred, average=\"macro\", zero_division=0),\n",
        "            \"recall\":    recall_score(y_true, y_pred, average=\"macro\"),\n",
        "        }\n",
        "        print(f\"Accuracy={metrics['accuracy']:.4f} | F1(macro)={metrics['f1']:.4f} | \"\n",
        "              f\"Precision(macro)={metrics['precision']:.4f} | Recall(macro)={metrics['recall']:.4f}\")\n",
        "        return metrics\n",
        "    else:\n",
        "        print(\"Labels not found in dataloader batches; returned predictions only.\")\n",
        "        return {}\n",
        "\n",
        "# ======================\n",
        "# Run pruning workflow\n",
        "# ======================\n",
        "model = load_fp32_model_state(MODEL_DIR, MODEL_FILE)\n",
        "print(f\"Baseline state_dict size (dense): {state_dict_size_mb(model):.2f} MB\")\n",
        "\n",
        "targets, density, sparsity = apply_uniform_l1_ffn(\n",
        "    model, amount_per_layer=PRUNE_PER_LAYER, prune_classifier=PRUNE_CLASSIFIER\n",
        ")\n",
        "\n",
        "remove_pruning_masks(targets)\n",
        "print(f\"Pruned state_dict size (dense save): {state_dict_size_mb(model):.2f} MB\")\n",
        "\n",
        "torch.save(model.state_dict(), PRUNED_SD_SAVE_PATH)\n",
        "print(\"Saved pruned state_dict to:\", PRUNED_SD_SAVE_PATH)\n",
        "\n",
        "# ======================\n",
        "# Evaluate with your existing loader\n",
        "# ======================\n",
        "if \"test_loader_clean\" in globals():\n",
        "    print(\"\\nEvaluating PRUNED model on test_loader_clean (CPU)...\")\n",
        "    pruned_metrics = evaluate_model_cpu(model, test_loader_clean, label_names=ordered_labels)\n",
        "    print(\"Test metrics:\", pruned_metrics)\n",
        "else:\n",
        "    print(\"Skipping evaluation: `test_loader_clean` not found in globals().\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UmzMU5Uuw6P",
        "outputId": "91f68109-0753-47da-a1b5-976ea7a94b45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline state_dict size (dense): 475.58 MB\n",
            "Applied FFN-only uniform L1: per-layer amount=0.22 | density=78.00% | sparsity=22.00%\n",
            "Pruned state_dict size (dense save): 475.58 MB\n",
            "Saved pruned state_dict to: /content/drive/MyDrive/deep_learning/HF_best_model_stage3_YR_ffn_uniform22_sd.pt\n",
            "\n",
            "Evaluating PRUNED model on test_loader_clean (CPU)...\n",
            "\n",
            "=== Classification Report ===\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            "Extremely Negative     0.9327    0.7720    0.8447       592\n",
            "          Negative     0.7940    0.8886    0.8386      1041\n",
            "           Neutral     0.8798    0.8514    0.8654       619\n",
            "          Positive     0.7666    0.8532    0.8076       947\n",
            "Extremely Positive     0.9041    0.7396    0.8136       599\n",
            "\n",
            "          accuracy                         0.8320      3798\n",
            "         macro avg     0.8554    0.8209    0.8340      3798\n",
            "      weighted avg     0.8401    0.8320    0.8322      3798\n",
            "\n",
            "Accuracy=0.8320 | F1(macro)=0.8340 | Precision(macro)=0.8554 | Recall(macro)=0.8209\n",
            "Test metrics: {'accuracy': 0.832016850974197, 'f1': 0.8339787215204026, 'precision': 0.8554258383713039, 'recall': 0.8209375932292252}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Knowledge Distillation Demo"
      ],
      "metadata": {
        "id": "Pz1_5z-roP2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== KD with RoBERTa teacher + DistilRoBERTa student (works with your TextClsDataset) =====\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
        "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments, TrainerCallback\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ---------- Datasets (build from your DataFrames) ----------\n",
        "NUM_LABELS = len(ORDERED_LABELS)\n",
        "\n",
        "train_ds = TextClsDataset(train_df, text_col=\"ProcessedTweet\", label_col=\"label\")\n",
        "val_ds   = TextClsDataset(eval_df,   text_col=\"ProcessedTweet\", label_col=\"label\")\n",
        "test_ds  = TextClsDataset(test_df,   text_col=\"ProcessedTweet\", label_col=\"label\")  # for final report\n",
        "\n",
        "# ---------- Teacher: RoBERTa (frozen) ----------\n",
        "BACKBONE_TEACHER = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "TEACHER_CKPT     = data_dir /\"HF_best_model_stage3_YR.pt\"  # <-- your RoBERTa weights\n",
        "\n",
        "def load_fp32_teacher_roberta(path, num_labels):\n",
        "    # RobertaWithDropout must be defined (the CLS-based version)\n",
        "    model = RobertaWithDropout(BACKBONE_TEACHER, num_labels=num_labels, dropout_rate=0.2)\n",
        "    state = torch.load(path, map_location=\"cpu\")  # expects state_dict\n",
        "    model.load_state_dict(state, strict=True)\n",
        "    model.eval()\n",
        "    for p in model.parameters(): p.requires_grad = False\n",
        "    return model\n",
        "\n",
        "teacher = load_fp32_teacher_roberta(TEACHER_CKPT, NUM_LABELS).to(device)\n",
        "\n",
        "# ---------- Student: DistilRoBERTa ----------\n",
        "student = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilroberta-base\",\n",
        "    num_labels=NUM_LABELS\n",
        ").to(device)\n",
        "\n",
        "# ---------- Metrics (add micro/weighted for epoch table) ----------\n",
        "def compute_metrics_fn(eval_pred):\n",
        "    preds  = getattr(eval_pred, \"predictions\", eval_pred[0])\n",
        "    labels = getattr(eval_pred, \"label_ids\",    eval_pred[1])\n",
        "    if isinstance(preds, tuple): preds = preds[0]\n",
        "    preds = np.argmax(preds, axis=-1)\n",
        "    return {\n",
        "        \"accuracy\":      accuracy_score(labels, preds),\n",
        "        \"f1_macro\":      f1_score(labels, preds, average=\"macro\"),\n",
        "        \"f1_micro\":      f1_score(labels, preds, average=\"micro\"),\n",
        "        \"f1_weighted\":   f1_score(labels, preds, average=\"weighted\"),\n",
        "        \"precision\":     precision_score(labels, preds, average=\"macro\", zero_division=0),\n",
        "        \"recall\":        recall_score(labels, preds, average=\"macro\"),\n",
        "    }\n",
        "\n",
        "# ---------- KD Trainer ----------\n",
        "class DistillationTrainer(Trainer):\n",
        "    def __init__(self, *args, teacher_model=None, temperature=2.0, alpha=0.7, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        assert teacher_model is not None, \"teacher_model must be provided\"\n",
        "        self.teacher = teacher_model\n",
        "        self.temperature = temperature\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        # RoBERTa/DistilRoBERTa do not use token_type_ids\n",
        "        s_inputs = dict(inputs)\n",
        "        s_inputs.pop(\"token_type_ids\", None)\n",
        "\n",
        "        out_s = model(**s_inputs)\n",
        "        logits_student = out_s.logits\n",
        "\n",
        "        with torch.no_grad():\n",
        "            t_out = self.teacher(\n",
        "                input_ids=s_inputs[\"input_ids\"],\n",
        "                attention_mask=s_inputs.get(\"attention_mask\", None)\n",
        "            )\n",
        "            logits_teacher = t_out[\"logits\"] if isinstance(t_out, dict) else t_out.logits\n",
        "\n",
        "        loss_ce = F.cross_entropy(logits_student, inputs[\"labels\"])\n",
        "\n",
        "        T = self.temperature\n",
        "        loss_kl = F.kl_div(\n",
        "            F.log_softmax(logits_student / T, dim=-1),\n",
        "            F.softmax(logits_teacher / T, dim=-1),\n",
        "            reduction=\"batchmean\"\n",
        "        ) * (T ** 2)\n",
        "\n",
        "        loss = self.alpha * loss_ce + (1.0 - self.alpha) * loss_kl\n",
        "        return (loss, out_s) if return_outputs else loss\n",
        "\n",
        "# ---------- Pretty per-epoch table ----------\n",
        "class EpochTableCallback(TrainerCallback):\n",
        "    def __init__(self):\n",
        "        self._printed_header = False\n",
        "        self.rows = []\n",
        "\n",
        "    def _last_train_loss_for_epoch(self, state, epoch_int):\n",
        "        for rec in reversed(state.log_history):\n",
        "            if rec.get(\"epoch\") is None: continue\n",
        "            if int(round(rec[\"epoch\"])) == epoch_int and \"loss\" in rec:\n",
        "                return float(rec[\"loss\"])\n",
        "        return float(\"nan\")\n",
        "\n",
        "    def on_epoch_end(self, args, state, control, **kwargs):\n",
        "        control.should_evaluate = True  # force eval at each epoch end\n",
        "\n",
        "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
        "        from math import isnan\n",
        "        ep = int(round(metrics.get(\"epoch\", state.epoch or 0))) or (len(self.rows) + 1)\n",
        "        tr_loss = self._last_train_loss_for_epoch(state, ep)\n",
        "        row = {\n",
        "            \"Epoch\": ep,\n",
        "            \"Training Loss\": tr_loss,\n",
        "            \"Validation Loss\": float(metrics.get(\"eval_loss\", \"nan\")),\n",
        "            \"Accuracy\": float(metrics.get(\"eval_accuracy\", \"nan\")),\n",
        "            \"F1 Micro\": float(metrics.get(\"eval_f1_micro\", \"nan\")),\n",
        "            \"F1 Macro\": float(metrics.get(\"eval_f1_macro\", \"nan\")),\n",
        "            \"F1 Weighted\": float(metrics.get(\"eval_f1_weighted\", \"nan\")),\n",
        "        }\n",
        "        self.rows.append(row)\n",
        "        if not self._printed_header:\n",
        "            print(\"\\nEpoch  Training Loss  Validation Loss   Accuracy   F1 Micro   F1 Macro  F1 Weighted\")\n",
        "            self._printed_header = True\n",
        "        def f(x):\n",
        "            from math import isnan\n",
        "            return \"   nan\" if (x != x) or isnan(x) else f\"{x:10.6f}\"\n",
        "        print(f\"{row['Epoch']:>5}  {f(row['Training Loss'])}   {f(row['Validation Loss'])}  \"\n",
        "              f\"{f(row['Accuracy'])}  {f(row['F1 Micro'])}  {f(row['F1 Macro'])}  {f(row['F1 Weighted'])}\")\n",
        "\n",
        "epoch_table = EpochTableCallback()\n",
        "\n",
        "# ---------- TrainingArguments (minimal; callback handles per-epoch eval) ----------\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./distill_results_stage3_YR_student\",\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=64,\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    report_to=\"none\",\n",
        "    remove_unused_columns=False,  # keep dict keys from our dataset\n",
        ")\n",
        "\n",
        "# ---------- Trainer ----------\n",
        "trainer_distill = DistillationTrainer(\n",
        "    model=student,\n",
        "    teacher_model=teacher,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    compute_metrics=compute_metrics_fn,\n",
        "    data_collator=collator,  # your collator (RoBERTa)\n",
        ")\n",
        "\n",
        "trainer_distill.add_callback(epoch_table)\n",
        "\n",
        "# ---------- Train ----------\n",
        "trainer_distill.train()\n",
        "\n",
        "# ---------- End-of-run multi-line Classification Report on TEST ----------\n",
        "pred = trainer_distill.predict(test_ds)  # uses the same collator internally\n",
        "y_true = pred.label_ids\n",
        "y_pred = pred.predictions.argmax(axis=-1)\n",
        "\n",
        "print(\"\\n=== Classification Report ===\")\n",
        "print(classification_report(y_true, y_pred, target_names=ORDERED_LABELS, digits=4))\n",
        "\n",
        "# ---------- Save student ----------\n",
        "trainer_distill.save_model(\"./distill_results_stage3_YR_student/best_student\")\n",
        "print(\"\\nDistillation complete. Student (DistilRoBERTa) model saved.\")\n",
        "print(\"Student parameter count:\", sum(p.numel() for p in student.parameters()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cwSHpnBEoPhn",
        "outputId": "74c49595-69b4-456f-c85e-9d65a0b4e6ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5145' max='5145' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5145/5145 06:26, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1 Macro</th>\n",
              "      <th>F1 Micro</th>\n",
              "      <th>F1 Weighted</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.509000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.096800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.829000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.587000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.434600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.387500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>1.372600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.284800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>1.239300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.222100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>1.179000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.134100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>1.117000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.063500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>1.051900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.979900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.948700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>1.001300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>0.948300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.973300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1029</td>\n",
              "      <td>0.973300</td>\n",
              "      <td>0.816348</td>\n",
              "      <td>0.769679</td>\n",
              "      <td>0.779466</td>\n",
              "      <td>0.769679</td>\n",
              "      <td>0.769256</td>\n",
              "      <td>0.779847</td>\n",
              "      <td>0.789042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>0.819700</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.703100</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>0.719900</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.776200</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>0.724600</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.746200</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>0.777100</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.731800</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1450</td>\n",
              "      <td>0.726100</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.694700</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1550</td>\n",
              "      <td>0.729000</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.741200</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1650</td>\n",
              "      <td>0.689600</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.723400</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1750</td>\n",
              "      <td>0.728100</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.658900</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1850</td>\n",
              "      <td>0.651200</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.654400</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1950</td>\n",
              "      <td>0.643300</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.614000</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2050</td>\n",
              "      <td>0.666600</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2058</td>\n",
              "      <td>0.666600</td>\n",
              "      <td>0.661543</td>\n",
              "      <td>0.819485</td>\n",
              "      <td>0.824585</td>\n",
              "      <td>0.819485</td>\n",
              "      <td>0.819114</td>\n",
              "      <td>0.820984</td>\n",
              "      <td>0.834871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.525400</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2150</td>\n",
              "      <td>0.462200</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.507500</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2250</td>\n",
              "      <td>0.545700</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>0.522100</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2350</td>\n",
              "      <td>0.518700</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.510900</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2450</td>\n",
              "      <td>0.511900</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.486000</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2550</td>\n",
              "      <td>0.527600</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>0.512200</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2650</td>\n",
              "      <td>0.457400</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>0.496400</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2750</td>\n",
              "      <td>0.471900</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>0.554900</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2850</td>\n",
              "      <td>0.505400</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>0.453200</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2950</td>\n",
              "      <td>0.491000</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.449800</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3050</td>\n",
              "      <td>0.445800</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3087</td>\n",
              "      <td>0.445800</td>\n",
              "      <td>0.672272</td>\n",
              "      <td>0.813897</td>\n",
              "      <td>0.819562</td>\n",
              "      <td>0.813897</td>\n",
              "      <td>0.812868</td>\n",
              "      <td>0.817387</td>\n",
              "      <td>0.833031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3100</td>\n",
              "      <td>0.448700</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3150</td>\n",
              "      <td>0.365000</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3200</td>\n",
              "      <td>0.409500</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3250</td>\n",
              "      <td>0.343900</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3300</td>\n",
              "      <td>0.351000</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3350</td>\n",
              "      <td>0.377000</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3400</td>\n",
              "      <td>0.357300</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3450</td>\n",
              "      <td>0.349800</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.364000</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3550</td>\n",
              "      <td>0.406600</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3600</td>\n",
              "      <td>0.393300</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3650</td>\n",
              "      <td>0.330900</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3700</td>\n",
              "      <td>0.402100</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3750</td>\n",
              "      <td>0.346400</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3800</td>\n",
              "      <td>0.394800</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3850</td>\n",
              "      <td>0.348900</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3900</td>\n",
              "      <td>0.360000</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3950</td>\n",
              "      <td>0.362400</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.336200</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4050</td>\n",
              "      <td>0.342200</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4100</td>\n",
              "      <td>0.334600</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4116</td>\n",
              "      <td>0.334600</td>\n",
              "      <td>0.739040</td>\n",
              "      <td>0.818270</td>\n",
              "      <td>0.823273</td>\n",
              "      <td>0.818270</td>\n",
              "      <td>0.817714</td>\n",
              "      <td>0.818721</td>\n",
              "      <td>0.840223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4150</td>\n",
              "      <td>0.321100</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4200</td>\n",
              "      <td>0.281100</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4250</td>\n",
              "      <td>0.268900</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4300</td>\n",
              "      <td>0.298800</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4350</td>\n",
              "      <td>0.253400</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4400</td>\n",
              "      <td>0.298600</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4450</td>\n",
              "      <td>0.299000</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.278600</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4550</td>\n",
              "      <td>0.282300</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4600</td>\n",
              "      <td>0.267200</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4650</td>\n",
              "      <td>0.295200</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4700</td>\n",
              "      <td>0.277200</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4750</td>\n",
              "      <td>0.230600</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4800</td>\n",
              "      <td>0.275200</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4850</td>\n",
              "      <td>0.279200</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4900</td>\n",
              "      <td>0.298400</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4950</td>\n",
              "      <td>0.280200</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.282700</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5050</td>\n",
              "      <td>0.279500</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5100</td>\n",
              "      <td>0.271900</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5145</td>\n",
              "      <td>0.271900</td>\n",
              "      <td>0.609968</td>\n",
              "      <td>0.846453</td>\n",
              "      <td>0.850710</td>\n",
              "      <td>0.846453</td>\n",
              "      <td>0.846310</td>\n",
              "      <td>0.846947</td>\n",
              "      <td>0.858182</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch  Training Loss  Validation Loss   Accuracy   F1 Micro   F1 Macro  F1 Weighted\n",
            "    1    0.973300     0.816348    0.769679    0.769679    0.779466    0.769256\n",
            "    2    0.666600     0.661543    0.819485    0.819485    0.824585    0.819114\n",
            "    3    0.445800     0.672272    0.813897    0.813897    0.819562    0.812868\n",
            "    4    0.334600     0.739040    0.818270    0.818270    0.823273    0.817714\n",
            "    5    0.271900     0.609968    0.846453    0.846453    0.850710    0.846310\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Classification Report ===\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            "Extremely Negative     0.7956    0.9071    0.8477       592\n",
            "          Negative     0.8173    0.7819    0.7992      1041\n",
            "           Neutral     0.9286    0.8191    0.8704       619\n",
            "          Positive     0.8034    0.7941    0.7987       947\n",
            "Extremely Positive     0.8202    0.8831    0.8505       599\n",
            "\n",
            "          accuracy                         0.8265      3798\n",
            "         macro avg     0.8330    0.8371    0.8333      3798\n",
            "      weighted avg     0.8290    0.8265    0.8263      3798\n",
            "\n",
            "\n",
            "Distillation complete. Student (DistilRoBERTa) model saved.\n",
            "Student parameter count: 82122245\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "76ee334621b847c5a00c72f02c592140": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_27deb97279b64e6fa620fb29a0612092",
              "IPY_MODEL_b598d84eebc5474385c6393605c56166",
              "IPY_MODEL_8a34f293210847ad82eb180613be13fc"
            ],
            "layout": "IPY_MODEL_5d8365d57df1495e8f75cd7bf7c73dc2"
          }
        },
        "27deb97279b64e6fa620fb29a0612092": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a28428c6a5942fc9007078159e67270",
            "placeholder": "​",
            "style": "IPY_MODEL_aa0fc39bc74c4c2bbb9a0237be904598",
            "value": "config.json: 100%"
          }
        },
        "b598d84eebc5474385c6393605c56166": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_670d9cd294d54d92b92c28ef7770fb62",
            "max": 929,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_34959b2ff61643acb04b9400aba81963",
            "value": 929
          }
        },
        "8a34f293210847ad82eb180613be13fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9818c29beea4bfb8c47c1edab98b8f4",
            "placeholder": "​",
            "style": "IPY_MODEL_b3c2f6cb3e1b4e3a8a6a602c0c7e9980",
            "value": " 929/929 [00:00&lt;00:00, 112kB/s]"
          }
        },
        "5d8365d57df1495e8f75cd7bf7c73dc2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a28428c6a5942fc9007078159e67270": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa0fc39bc74c4c2bbb9a0237be904598": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "670d9cd294d54d92b92c28ef7770fb62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34959b2ff61643acb04b9400aba81963": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f9818c29beea4bfb8c47c1edab98b8f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3c2f6cb3e1b4e3a8a6a602c0c7e9980": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9aa44bd9f13d4ea4ac03e2a9c74b34de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fc82393b41b44e07bb3d32174eaf9439",
              "IPY_MODEL_70c117d9990f4f129db38d0d8225fe45",
              "IPY_MODEL_03d0b2edf5a5427eb79a861e57a07cc1"
            ],
            "layout": "IPY_MODEL_3eba577e5a044ca2bc7deb8dff0cf7dd"
          }
        },
        "fc82393b41b44e07bb3d32174eaf9439": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ae5f4a751c248099d03f1f1c1151ed8",
            "placeholder": "​",
            "style": "IPY_MODEL_5ca4a5dfb1a448ecae0504c0606f1ea2",
            "value": "vocab.json: "
          }
        },
        "70c117d9990f4f129db38d0d8225fe45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6ba994f9887463a93f130314396a985",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f3ff21716a8d49efac24150813073337",
            "value": 1
          }
        },
        "03d0b2edf5a5427eb79a861e57a07cc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6699ab8fe8b64819bffef780afb34713",
            "placeholder": "​",
            "style": "IPY_MODEL_fb38fbac642d4e258cbf4327a6fb204f",
            "value": " 899k/? [00:00&lt;00:00, 7.25MB/s]"
          }
        },
        "3eba577e5a044ca2bc7deb8dff0cf7dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ae5f4a751c248099d03f1f1c1151ed8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ca4a5dfb1a448ecae0504c0606f1ea2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a6ba994f9887463a93f130314396a985": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "f3ff21716a8d49efac24150813073337": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6699ab8fe8b64819bffef780afb34713": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb38fbac642d4e258cbf4327a6fb204f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee9fb060ecba40c3b7f3fd7ee99c8a40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1690d53a5d754b3ca4a8970f891de51c",
              "IPY_MODEL_1dad298ef42043f4a75af67417f4851a",
              "IPY_MODEL_dbe8d41bc1b5493b9c99940448fbde2c"
            ],
            "layout": "IPY_MODEL_3418b74d305c47d5bfe63da49e98194e"
          }
        },
        "1690d53a5d754b3ca4a8970f891de51c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41192a2872d945b5ba940ad35509336a",
            "placeholder": "​",
            "style": "IPY_MODEL_fa631f1422624295841a82b24566f513",
            "value": "merges.txt: "
          }
        },
        "1dad298ef42043f4a75af67417f4851a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e90920f8751b4ac989844af98b47861d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a3a6a9b0810c48a4a447acd37363c68e",
            "value": 1
          }
        },
        "dbe8d41bc1b5493b9c99940448fbde2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37b89f0746dc42abadbee6048820a041",
            "placeholder": "​",
            "style": "IPY_MODEL_d9d2a863a10f40e5b91d285c262d2071",
            "value": " 456k/? [00:00&lt;00:00, 29.4MB/s]"
          }
        },
        "3418b74d305c47d5bfe63da49e98194e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41192a2872d945b5ba940ad35509336a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa631f1422624295841a82b24566f513": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e90920f8751b4ac989844af98b47861d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "a3a6a9b0810c48a4a447acd37363c68e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "37b89f0746dc42abadbee6048820a041": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9d2a863a10f40e5b91d285c262d2071": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a960ace2d0d42c7a4d1043901cb446a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_90e8d7dc8c1b4b7bb39a583eda0bef60",
              "IPY_MODEL_e58b57b389824f1c9a97d84070e6101e",
              "IPY_MODEL_0d8997ed71794f99a028ef3d27c013b8"
            ],
            "layout": "IPY_MODEL_fb37b18cb0b14ce0b1c75179bf3ebdc1"
          }
        },
        "90e8d7dc8c1b4b7bb39a583eda0bef60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96078218617a48189755b555bc0c5bab",
            "placeholder": "​",
            "style": "IPY_MODEL_da4c30f5713545358ee36d23fbc87550",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "e58b57b389824f1c9a97d84070e6101e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5df14c0a1e7f45b298bf16ea0ff098f4",
            "max": 239,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c9263bf9854a4d38a4c94f7c81c0d6db",
            "value": 239
          }
        },
        "0d8997ed71794f99a028ef3d27c013b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7f91a776a084bda86ea96a7c2d0981b",
            "placeholder": "​",
            "style": "IPY_MODEL_ce2b967d58744c849e16d92810a28929",
            "value": " 239/239 [00:00&lt;00:00, 26.8kB/s]"
          }
        },
        "fb37b18cb0b14ce0b1c75179bf3ebdc1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96078218617a48189755b555bc0c5bab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da4c30f5713545358ee36d23fbc87550": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5df14c0a1e7f45b298bf16ea0ff098f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9263bf9854a4d38a4c94f7c81c0d6db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a7f91a776a084bda86ea96a7c2d0981b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce2b967d58744c849e16d92810a28929": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e6243b94890465c940a1da30a9b26b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6d5585ff278541b4968f8b346ba7c2d5",
              "IPY_MODEL_022f704fa8444eb7abec242c44f3b816",
              "IPY_MODEL_b907967cb0c74542bac4b7a4c157d444"
            ],
            "layout": "IPY_MODEL_2b28c66b3daa4337adff34d4836166ae"
          }
        },
        "6d5585ff278541b4968f8b346ba7c2d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ad3328082fd4226acc0a470c3ce5feb",
            "placeholder": "​",
            "style": "IPY_MODEL_0a51cd802ca048fd9ecd486aeb7f5ffc",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "022f704fa8444eb7abec242c44f3b816": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e212984111f3420a92845f7231561c53",
            "max": 501045531,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_71c35030b6db4890a649ece5b6ffa902",
            "value": 501045531
          }
        },
        "b907967cb0c74542bac4b7a4c157d444": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d098613fdc894d89960e3cfbd2080a18",
            "placeholder": "​",
            "style": "IPY_MODEL_c52ed5e048284f98a7088539575a5545",
            "value": " 501M/501M [00:01&lt;00:00, 347MB/s]"
          }
        },
        "2b28c66b3daa4337adff34d4836166ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ad3328082fd4226acc0a470c3ce5feb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a51cd802ca048fd9ecd486aeb7f5ffc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e212984111f3420a92845f7231561c53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71c35030b6db4890a649ece5b6ffa902": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d098613fdc894d89960e3cfbd2080a18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c52ed5e048284f98a7088539575a5545": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "00e508b34d784c2083020095643af3ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2c52c92f8d3b4dbbab696a147614d5cc",
              "IPY_MODEL_5969ac09803b4a4691836eeca02bc910",
              "IPY_MODEL_0a21e481f5824d7da7b7ffd6581d2002"
            ],
            "layout": "IPY_MODEL_79559d31e1214c9085ff5b96333569ee"
          }
        },
        "2c52c92f8d3b4dbbab696a147614d5cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b9a339706c64e78a26dbc160869b150",
            "placeholder": "​",
            "style": "IPY_MODEL_fb1bcec808ba450a818c88e0bedc014a",
            "value": "model.safetensors: 100%"
          }
        },
        "5969ac09803b4a4691836eeca02bc910": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1100d8d5db9d4cf9a5a022947e9e4d19",
            "max": 500982668,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8b0021002aa3447e88db198840667690",
            "value": 500982668
          }
        },
        "0a21e481f5824d7da7b7ffd6581d2002": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1af77345a4fa41a8af4ed1693d589d5e",
            "placeholder": "​",
            "style": "IPY_MODEL_a062d61d2b71462c9813a039420d3fff",
            "value": " 501M/501M [00:02&lt;00:00, 314MB/s]"
          }
        },
        "79559d31e1214c9085ff5b96333569ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b9a339706c64e78a26dbc160869b150": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb1bcec808ba450a818c88e0bedc014a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1100d8d5db9d4cf9a5a022947e9e4d19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b0021002aa3447e88db198840667690": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1af77345a4fa41a8af4ed1693d589d5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a062d61d2b71462c9813a039420d3fff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}