{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16661,"status":"ok","timestamp":1755548967100,"user":{"displayName":"yarden shalom","userId":"10505379448441178663"},"user_tz":-180},"id":"h8fwlj7vYQBb","outputId":"45b9e50a-a7fa-4ee6-e3b9-67323cb6caba"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# ========================================\n","# 1) Connect to Google Drive\n","# ========================================\n","#from google.colab import drive\n","#drive.mount('/content/drive')\n","\n","#MODEL_DIR = \"/content/drive/MyDrive/deep_learning\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":105612,"status":"ok","timestamp":1755549072714,"user":{"displayName":"yarden shalom","userId":"10505379448441178663"},"user_tz":-180},"id":"18LqQBe_YmqH","outputId":"644939a0-5b16-4687-cbf8-5c9b905350d3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.55.2)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.10.0)\n","Collecting evaluate\n","  Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\n","Collecting optuna\n","  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n","Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.21.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.4)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.6.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n","Collecting alembic>=1.5.0 (from optuna)\n","  Downloading alembic-1.16.4-py3-none-any.whl.metadata (7.3 kB)\n","Collecting colorlog (from optuna)\n","  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.43)\n","Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\n","Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.45)\n","Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n","Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.5)\n","Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.7)\n","Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.34.1)\n","Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.14.1)\n","Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n","Collecting nvidia-nccl-cu12==2.21.5 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n","Downloading evaluate-0.4.5-py3-none-any.whl (84 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading optuna-4.5.0-py3-none-any.whl (400 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading alembic-1.16.4-py3-none-any.whl (247 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n","Installing collected packages: nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, colorlog, nvidia-cusparse-cu12, nvidia-cudnn-cu12, alembic, optuna, nvidia-cusolver-cu12, evaluate\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-nccl-cu12\n","    Found existing installation: nvidia-nccl-cu12 2.23.4\n","    Uninstalling nvidia-nccl-cu12-2.23.4:\n","      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed alembic-1.16.4 colorlog-6.9.0 evaluate-0.4.5 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 optuna-4.5.0\n"]}],"source":["# ========================================\n","# 2) Install required libraries\n","# ========================================\n","!pip install -U transformers datasets accelerate evaluate optuna wandb\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3535,"status":"ok","timestamp":1755549076252,"user":{"displayName":"yarden shalom","userId":"10505379448441178663"},"user_tz":-180},"id":"pRlucwrcZzDm","outputId":"9c1b3123-614f-4caa-ce44-919702d1f989","collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.5)\n","Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.0.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n","Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n","Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.34.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.15)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.1)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.7)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.8.3)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.4)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n"]}],"source":["!pip install evaluate"]},{"cell_type":"code","source":["!pip install -U transformers\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8_lUu73RwbbY","executionInfo":{"status":"ok","timestamp":1755549079782,"user_tz":-180,"elapsed":3518,"user":{"displayName":"yarden shalom","userId":"10505379448441178663"}},"outputId":"854f2021-9636-4b94-98b1-ef965b45fdae","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.55.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.4)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.6.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IWSgKFUoYqzs"},"outputs":[],"source":["# ========================================\n","# 3) Import libraries\n","# ========================================\n","import os, time, copy, tempfile, inspect\n","import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","import torch, torch.nn as nn\n","from torch.quantization import quantize_dynamic\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n","import os\n","import re\n","import torch\n","import torch.nn as nn\n","import pandas as pd\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import AutoTokenizer, AutoModel, DataCollatorWithPadding\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1755549094064,"user":{"displayName":"yarden shalom","userId":"10505379448441178663"},"user_tz":-180},"id":"YQr3wGwNYsO7","outputId":"f5dc2bdc-fd86-43e0-cca9-83189f2b4d23"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}],"source":["# Check if GPU is available and select device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pBF3Ga4FY28M"},"outputs":[],"source":["# === Load data ===\n","#test_df = pd.read_csv('/content/drive/MyDrive/deep_learning/val_processed.csv', encoding='latin1')\n","import pandas as pd\n","\n","# Navigate from model folder to data folder\n","current_dir = Path.cwd()  # models/bert-base-uncased/\n","models_dir = current_dir.parent  # models/\n","project_root = models_dir.parent  # project root\n","data_dir = models_dir / 'data'\n","\n","# Load data\n","train_df = pd.read_csv(data_dir / 'train_processed.csv', encoding='latin1')\n","eval_df = pd.read_csv(data_dir / 'val_processed.csv', encoding='latin1')\n","test_df = pd.read_csv(data_dir / 'test_processed.csv', encoding='latin1')\n","\n","\n","print(f\"Test shape: {test_df.shape}\")\n","# Define label order\n","ordered_labels = ['Extremely Negative', 'Negative', 'Neutral', 'Positive', 'Extremely Positive']\n","label2id = {lbl:i for i,lbl in enumerate(ordered_labels)}\n","id2label = {i:lbl for lbl,i in label2id.items()}\n"]},{"cell_type":"code","source":["# --------- config ----------\n","#MODEL_DIR  = \"/content/drive/MyDrive/deep_learning\"\n","MODEL_NAME = \"bert-base-uncased\"\n","MODEL_DIR  =data_dir\n"],"metadata":{"id":"GS-qhX-8EdYH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_df[\"label\"] = test_df[\"Sentiment\"].map(label2id)"],"metadata":{"id":"HUgOIRMPMi5f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def clean_for_cardiffnlp(text):\n","    if pd.isnull(text):\n","        return \"\"\n","\n","    tokens = []\n","    for t in text.split(\" \"):\n","        if t.startswith(\"@\") and len(t) > 1:\n","            tokens.append(\"@user\")\n","        elif t.startswith(\"http\"):\n","            tokens.append(\"http\")\n","        else:\n","            tokens.append(t)\n","    text = \" \".join(tokens)\n","\n","    # Normalize common COVID variants to \"covid\"\n","    text = re.sub(r\"\\b(coronaviruspandemic|covid[_\\s-]*2019|covid[_\\s-]*19|covid2019|coronavirus2019|coronavirus|corona)\\b\", \"covid\", text, flags=re.IGNORECASE)\n","\n","    # Decode HTML entities\n","    text = html.unescape(text)\n","\n","    # Normalize whitespace and repeated punctuation (optional)\n","    text = re.sub(r\"\\s+\", \" \", text).strip()\n","    text = re.sub(r\"(\\.\\s*){2,}\", \". \", text)\n","    text = re.sub(r\"([!?]){2,}\", r\"\\1\", text)\n","    text = re.sub(r\"(\\?\\s+){2,}\", \"?\", text)\n","    text = re.sub(r\"(\\!\\s+){2,}\", \"!\", text)\n","\n","    return text\n"],"metadata":{"id":"I54afjFfMjUB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["BASE_CHECKPOINT = \"bert-base-uncased\"\n","LABEL_COL = \"label\"\n","BATCH_SIZE = 128\n","NUM_WORKERS = 2\n","PIN_MEMORY = True\n","\n","\n","# ==== tokenizer + collator ====\n","tokenizer = AutoTokenizer.from_pretrained(BASE_CHECKPOINT, use_fast=True)\n","collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","# ==== Class Dataset ====\n","class TextClsDataset(Dataset):\n","    def __init__(self, df, text_col, label_col):\n","        self.texts = df[text_col].astype(str).tolist()\n","        self.labels = df[label_col].astype(int).tolist()\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        enc = tokenizer(\n","            self.texts[idx],\n","            truncation=True,\n","            max_length=128,   # לשנות אם צריך\n","            return_tensors=None\n","        )\n","        enc[\"labels\"] = self.labels[idx]\n","        return enc\n","\n","# prepere datasets\n","test_ds_clean = TextClsDataset(test_df, text_col=\"ProcessedTweet\",        label_col=LABEL_COL)\n","#test_ds_light = TextClsDataset(test_df, text_col=\"ProcessedTweet_light\",  label_col=LABEL_COL)\n","\n","# DataLoaders\n","test_loader_clean = DataLoader(\n","    test_ds_clean,\n","    batch_size=BATCH_SIZE,\n","    shuffle=False,\n","    num_workers=NUM_WORKERS,\n","    pin_memory=PIN_MEMORY,\n","    collate_fn=collator\n",")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":256,"referenced_widgets":["8c95e2cb5e4d4a10a83bd9defa312775","b632f5376ff34502ade19db3347fb0ff","f8337816508a408b8c0a768652890e08","00871705415647cfa2384868f0eb33dc","87910b9892c947169b312522083df579","aabc15de5c0242ddada3f287b03d5699","1a563ff4992b424b8b066087dc36ce81","abda4339897b448ca23d4a16d9b282e0","60e252aa12884149acdce7c935f8e07c","1822934776c74d288f51ee6c252deff1","a044273c11bb4d30ae37dec26fb257a3","48ac50f872924ce2a8e55cb2c3f418ed","904a13e36c7d4db490adbd03da04dca6","25f224dd8d0e48ca9a0038f3e2b38b15","3ca8b9f99fba4a6bbc8246cb4ae6b4a5","118df6fe52dd433393fe94013ea4a543","37fdb3093fb14840b660b57a99c2a1f0","ca8a28e6d88f44cbb37f359adbe85b5e","021b01e3e204444ca337de51a7a717b7","8a0942347ee242ed80ce449a9444d752","3d9bcd9a56a34d35a4ccae050cbca4d1","4617ee04f4b24d5281959de6078d1798","0aacb8bd53804d749767ff3c51962850","32bf877344724e29b0341a89a60b076b","60962fd0f7894249bdd4f601ed57a9e5","94d4e8f8a2b548e7b91575f25c61a188","f9328d610f824ddbb1fabc36cf6a6e46","8898644770e946e7be3bd9a8555a39d6","a9b7d0bbb0034eb7aea3bd8987d0c8f6","d30a4ec32b41465c97b21d2095858f4a","505b443e9a1143c68791ac2629edf1a7","3a78dd164fa84840b3ca3d7cde6ec350","ce47ba4ee51f47d9a64056736d9d8809","e59c767d528b40c2b5356b7c77504095","7bbbfb39f14943b4975712b3c7db4708","ab87cd6113f447f9bd649959cbcce110","09982295b35d41c1a39f9b6e2dbf89db","b98a472d2dc0402b977646f02774e161","735a89ee5f024047a725a0c0a7034446","5f211fb13d6e4ae1a5b9e35ade6d613d","a5400e92082a4683a9c15e105960f543","a60e486faa644dae87d7946514af6b4d","1b5d264de3ad49e4a7a2c0095267681f","8db4272b28c64830a331d0853bde081b"]},"id":"yw_7XUGOMueZ","executionInfo":{"status":"ok","timestamp":1755549099008,"user_tz":-180,"elapsed":1604,"user":{"displayName":"yarden shalom","userId":"10505379448441178663"}},"outputId":"02c09d34-3989-4fa7-8c8f-e5217f272d5f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c95e2cb5e4d4a10a83bd9defa312775"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48ac50f872924ce2a8e55cb2c3f418ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0aacb8bd53804d749767ff3c51962850"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e59c767d528b40c2b5356b7c77504095"}},"metadata":{}}]},{"cell_type":"code","source":["# Use the same model class as in training\n","class ConfigurableBertModel(nn.Module):\n","    def __init__(self, model_name: str, num_labels: int, dropout_rate: float = 0.1):\n","        super(ConfigurableBertModel, self).__init__()\n","        self.encoder = AutoModel.from_pretrained(model_name)\n","        self.dropout = nn.Dropout(dropout_rate)\n","        self.classifier = nn.Linear(self.encoder.config.hidden_size, num_labels)\n","\n","    def forward(self, input_ids, attention_mask=None):\n","        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n","        pooled_output = outputs.last_hidden_state[:, 0]  # CLS token\n","        pooled_output = self.dropout(pooled_output)\n","        logits = self.classifier(pooled_output)\n","        return type('ModelOutput', (), {'logits': logits})()\n","\n"],"metadata":{"id":"eaKSDPKcNxmx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#MODEL_DIR = \"/content/drive/MyDrive/deep_learning\"\n","MODEL_DIR  =data_dir\n","\n","# Map -> preprocessing\n","MODEL_PREPROC = {\n","    \"best_BERT_base_uncasedmodel2.pt\": \"clean\",\n","}\n","\n","# Evaluation function\n","def evaluate_model(model, dataloader, device):\n","    model.eval()\n","    all_preds, all_labels = [], []\n","\n","    with torch.no_grad():\n","        for batch in dataloader:\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['labels'].to(device)\n","\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","            preds = torch.argmax(outputs.logits, dim=1)\n","\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    # Classification report\n","    print(\"Classification Report:\")\n","    print(classification_report(\n","        all_labels, all_preds,\n","        target_names=['Extremely Negative', 'Negative', 'Neutral', 'Positive', 'Extremely Positive']\n","    ))\n","\n","    return {\n","        \"accuracy\": accuracy_score(all_labels, all_preds),\n","        \"f1\": f1_score(all_labels, all_preds, average='macro'),\n","        \"precision\": precision_score(all_labels, all_preds, average='macro'),\n","        \"recall\": recall_score(all_labels, all_preds, average='macro')\n","    }\n","\n","\n","# Evaluate all saved models\n","def evaluate_all_models(model_dir, model_preproc_map, test_loader_clean, device):\n","    results = {}\n","\n","    for model_file, preproc_type in model_preproc_map.items():\n","        print(f\"\\n=== Evaluating {model_file} ({preproc_type}) ===\")\n","\n","        model_path = os.path.join(model_dir, model_file)\n","\n","        # IMPORTANT: use ConfigurableBertModel, same as in training\n","        model = ConfigurableBertModel(\n","            model_name=\"bert-base-uncased\",\n","            num_labels=5,\n","            dropout_rate=0.2  # you can change if needed\n","        )\n","        model.load_state_dict(torch.load(model_path, map_location=device))\n","        model.to(device)\n","\n","        if preproc_type == \"clean\":\n","            test_loader = test_loader_clean\n","        elif preproc_type == \"light\":\n","            test_loader = test_loader_light\n","        else:\n","            raise ValueError(f\"Unknown preprocessing type: {preproc_type}\")\n","\n","        metrics = evaluate_model(model, test_loader, device)\n","        results[model_file] = metrics\n","\n","    print(\"\\n=== Summary of All Models ===\")\n","    for model_file, metrics in results.items():\n","        print(f\"{model_file}: \"\n","              f\"Acc={metrics['accuracy']:.4f}, \"\n","              f\"F1={metrics['f1']:.4f}, \"\n","              f\"Precision={metrics['precision']:.4f}, \"\n","              f\"Recall={metrics['recall']:.4f}\")\n","\n","    return results\n"],"metadata":{"id":"Ww3lRyDoNn7i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results = evaluate_all_models(MODEL_DIR, MODEL_PREPROC, test_loader_clean, device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":381,"referenced_widgets":["e11787fde0d94c2696789ea5b1c22fc8","6997aaf43c0d48879e578183d5d32f2d","2f7543cf98fe4979abd518f461fb077d","ed14bd90d6b5432f80225d6ef469ae0b","eaea1b14c5af4484b2c93b3cdcfbb7b3","5eff5de808c54ed89ac1418c1314cde1","c552136156594ad89b2e57752ac9f374","0cded3539ab9412192ebb5c99ad42591","16338d707b4d48e0a69ce2f6674e71fb","9586bb098b4d495c82ed9d9ebb4dba35","ef553f8f64f14074bbe4fd9a9642a129"]},"id":"cUZylinYNYEZ","executionInfo":{"status":"ok","timestamp":1755549115345,"user_tz":-180,"elapsed":16275,"user":{"displayName":"yarden shalom","userId":"10505379448441178663"}},"outputId":"6bce7a32-4da7-475a-ec1f-c504c720710a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== Evaluating best_BERT_base_uncasedmodel2.pt (clean) ===\n"]},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e11787fde0d94c2696789ea5b1c22fc8"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Classification Report:\n","                    precision    recall  f1-score   support\n","\n","Extremely Negative       0.86      0.89      0.87       592\n","          Negative       0.85      0.85      0.85      1041\n","           Neutral       0.94      0.86      0.90       619\n","          Positive       0.83      0.83      0.83       947\n","Extremely Positive       0.85      0.88      0.86       599\n","\n","          accuracy                           0.86      3798\n","         macro avg       0.87      0.86      0.86      3798\n","      weighted avg       0.86      0.86      0.86      3798\n","\n","\n","=== Summary of All Models ===\n","best_BERT_base_uncasedmodel2.pt: Acc=0.8597, F1=0.8640, Precision=0.8655, Recall=0.8637\n"]}]},{"cell_type":"code","source":["BASE_CHECKPOINT = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n","LABEL_COL = \"label\"\n","BATCH_SIZE = 128\n","NUM_WORKERS = 2\n","PIN_MEMORY = True\n","\n","\n","# ==== tokenizer + collator ====\n","tokenizer = AutoTokenizer.from_pretrained(BASE_CHECKPOINT, use_fast=True)\n","collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","# ==== Class Dataset ====\n","class TextClsDataset(Dataset):\n","    def __init__(self, df, text_col, label_col):\n","        self.texts = df[text_col].astype(str).tolist()\n","        self.labels = df[label_col].astype(int).tolist()\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        enc = tokenizer(\n","            self.texts[idx],\n","            truncation=True,\n","            max_length=128,\n","            return_tensors=None\n","        )\n","        enc[\"labels\"] = self.labels[idx]\n","        return enc\n","\n","# prepere datasets\n","test_ds_clean = TextClsDataset(test_df, text_col=\"ProcessedTweet\",        label_col=LABEL_COL)\n","\n","# DataLoaders\n","test_loader_clean = DataLoader(\n","    test_ds_clean,\n","    batch_size=BATCH_SIZE,\n","    shuffle=False,\n","    num_workers=NUM_WORKERS,\n","    pin_memory=PIN_MEMORY,\n","    collate_fn=collator\n",")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["6557e17dc96c43a08a56658571ba913b","c7aa49f7b2874b55b4d30c37fdb4531c","2ecf254d59ba42b5a336367987ee4e73","8ec1bf9c437c420eaf7c941a6c141dfa","ed825e4e3f964d4897e65fb9ef624d09","97573ee09eee4f15bacd72ef8c4f38b9","83ac8c35c8ce4bcb8d9c0d723b7c1449","8b832ac05ec7479cb3f23e862d0d9b03","c0da58b8606243b6a5ed1f703988a23e","7e2e8e6efe5d4096880b932adb401833","c74ef0c3c7af496680d9f6ca54b66133","b666c1d18ca944cda298d3af0a6e2c04","90cdacfce6534f9c8d9d997c31002c98","0e0fe4e2e4af4fe4b4c710a2d99c3dc3","34515c1265194c22aed09047f8f2a7e1","065d66fa529d432d8b91a567099d7935","6d1d512b9278466aa728a98473069b5f","7e701d0514b04ba88eedc53091926459","7ef311adef084b8c9ff8ecc4d1d41d95","69ffe4ae7f7243d8a7f9c8d5c4543ba4","18a6bf95105349a88d2cf73cf236ea60","48388792736f4f4ca6aac639b173da59","d1b62b51ad694819b49402fb5c60dbb5","c2c0e9e9ddab4593b0d0f43342fef8c4","9a9c100dbb234f8b93c5a8197a1af0c0","1987b62301e442a49f878a9396247c58","aee670aaf4f54071ad945d83fd44019f","671a378e1e924b5ea6cb8c557cf63f9c","75e7e98d2e0f4fbe869b21862b66ab16","23827a28f24a4bb29154bd001ed4d95e","7599c511526e4597b9ffd2d6a9bf7bc5","693a164af51540debcfc43e9108f4abd","b891b3c2c70f478695f40741bb14e7bd","227aa49f05454ab0878c2ae582fc5c54","45ca09ebeb024678ade5cef952302fd3","c01fc89589d246f59e1fb3744b7b7c39","d6b9fcccdfba422b934d961c230f7b70","49e16c630eeb45b497599954df9f2aa0","775fb8b9636c4ee885ba27031fe55601","43161b51338642d0b72d9c0d90eff1db","d9f2139a490f4a3591b4f5f2f3995045","627b1077b52242b19e2e2a6b49b7e0c2","1cd7aa72cfcb48649465620c619e4e78","9e0479fd544b4e4a9124e91a664141fb"]},"id":"VKCr5hhYOkWy","executionInfo":{"status":"ok","timestamp":1755549116240,"user_tz":-180,"elapsed":888,"user":{"displayName":"yarden shalom","userId":"10505379448441178663"}},"outputId":"6a9ffdad-347f-4c40-93f4-1d461ed8906c"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/929 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6557e17dc96c43a08a56658571ba913b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b666c1d18ca944cda298d3af0a6e2c04"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1b62b51ad694819b49402fb5c60dbb5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"227aa49f05454ab0878c2ae582fc5c54"}},"metadata":{}}]},{"cell_type":"code","source":["import os\n","import re\n","import torch\n","import torch.nn as nn\n","from pathlib import Path\n","import pandas as pd\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import AutoTokenizer, AutoModel, DataCollatorWithPadding\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n","\n","# ==============================\n","# Config\n","# ==============================\n","MODEL_DIR  =data_dir\n","\n","BASE_CHECKPOINT = \"bert-base-uncased\"\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Evaluate these saved heads (your files)\n","MODEL_FILES = [\n","    \"HF_best_model_stage3.pt\",\n","]\n","\n","ORDERED_LABELS = ['Extremely Negative', 'Negative', 'Neutral', 'Positive', 'Extremely Positive']\n","LABEL2ID = {l: i for i, l in enumerate(ORDERED_LABELS)}\n","\n","# ==============================\n","# Tokenizer (+ special tokens)\n","# ==============================\n","tokenizer = AutoTokenizer.from_pretrained(BASE_CHECKPOINT, use_fast=True)\n","specials = {\"additional_special_tokens\": [\"<httpurl>\", \"<user>\", \"<hashtag>\", \"<emoji>\"]}\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True)\n","collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","\n","# ==============================\n","# Preprocessing (match training)\n","#   - URLs  -> <httpurl>\n","#   - @user -> <user>\n","#   - hashtags -> prefix with <hashtag> (keep the hashtag token)\n","#   - emojis -> prefix with <emoji> before the emoji cluster\n","# ==============================\n","_EMOJI_RE = re.compile(\n","    r'['\n","    r'\\U0001F1E0-\\U0001F1FF'  # flags\n","    r'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n","    r'\\U0001F600-\\U0001F64F'  # emoticons\n","    r'\\U0001F680-\\U0001F6FF'  # transport & map\n","    r'\\U0001F700-\\U0001F77F'\n","    r'\\U0001F780-\\U0001F7FF'\n","    r'\\U0001F800-\\U0001F8FF'\n","    r'\\U0001F900-\\U0001F9FF'\n","    r'\\U0001FA00-\\U0001FA6F'\n","    r'\\U0001FA70-\\U0001FAFF'\n","    r'\\u2600-\\u26FF\\u2700-\\u27BF'\n","    r']+'\n",")\n","\n","def preprocess_with_markers(t: str) -> str:\n","    if not isinstance(t, str):\n","        return \"\"\n","    t = re.sub(r'https?://\\S+', '<httpurl>', t)\n","    t = re.sub(r'@\\w+', '<user>', t)\n","    # add \"<hashtag> \" before hashtags that don't already have it\n","    t = re.sub(r'(?<!<hashtag>)#\\w+', lambda m: f\"<hashtag> {m.group(0)}\", t)\n","    # add \"<emoji> \" before emoji clusters\n","    t = _EMOJI_RE.sub(lambda m: f\"<emoji> {m.group(0)}\", t)\n","    t = re.sub(r'\\s+', ' ', t).strip()\n","    return t\n","\n","# ==============================\n","# Dataset\n","# ==============================\n","class TDataset(Dataset):\n","    def __init__(self, texts, labels, max_len=128):\n","        self.texts = texts\n","        self.labels = labels\n","        self.max_len = max_len\n","    def __len__(self):\n","        return len(self.texts)\n","    def __getitem__(self, i):\n","        enc = tokenizer(\n","            self.texts[i],\n","            truncation=True,\n","            max_length=self.max_len,\n","            return_tensors=\"pt\"\n","        )\n","        item = {k: v.squeeze(0) for k, v in enc.items()}\n","        item[\"labels\"] = torch.tensor(self.labels[i], dtype=torch.long)\n","        return item\n","\n","# ==============================\n","# Model (same head you trained)\n","# ==============================\n","class BertWithDropout(nn.Module):\n","    def __init__(self, model_name, num_labels=5, dropout_rate=0.2):\n","        super().__init__()\n","        self.backbone = AutoModel.from_pretrained(model_name)\n","        self.dropout  = nn.Dropout(dropout_rate)\n","        self.classifier = nn.Linear(self.backbone.config.hidden_size, num_labels)\n","    def forward(self, input_ids, attention_mask=None, labels=None, token_type_ids=None):\n","        outputs = self.backbone(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids\n","        )\n","        cls = outputs.last_hidden_state[:, 0, :]\n","        logits = self.classifier(self.dropout(cls))\n","        return {\"logits\": logits}\n","\n","# ==============================\n","# Evaluation loop\n","# ==============================\n","def evaluate_model(model, dataloader):\n","    model.eval()\n","    all_preds, all_labels = [], []\n","    with torch.no_grad():\n","        for batch in dataloader:\n","            ids   = batch[\"input_ids\"].to(DEVICE)\n","            mask  = batch.get(\"attention_mask\")\n","            ttids = batch.get(\"token_type_ids\")  # BERT supplies these\n","            if mask  is not None:  mask  = mask.to(DEVICE)\n","            if ttids is not None: ttids = ttids.to(DEVICE)\n","\n","            labels = batch[\"labels\"].cpu().numpy()\n","            logits = model(ids, attention_mask=mask, token_type_ids=ttids)[\"logits\"]\n","            preds  = torch.argmax(logits, dim=1).cpu().numpy()\n","            all_preds.extend(preds)\n","            all_labels.extend(labels)\n","\n","    print(classification_report(all_labels, all_preds, target_names=ORDERED_LABELS, digits=4))\n","    return {\n","        \"accuracy\":  accuracy_score(all_labels, all_preds),\n","        \"f1\":        f1_score(all_labels, all_preds, average='macro'),\n","        \"precision\": precision_score(all_labels, all_preds, average='macro'),\n","        \"recall\":    recall_score(all_labels, all_preds, average='macro'),\n","    }\n","\n","# ==============================\n","# Prepare test data and run\n","#   Expects: test_df with columns [\"OriginalTweet\",\"Sentiment\"]\n","# ==============================\n","test_df[\"label\"] = test_df[\"Sentiment\"].map(LABEL2ID)\n","\n","texts  = [preprocess_with_markers(x) for x in test_df[\"OriginalTweet\"].astype(str)]\n","labels = test_df[\"label\"].tolist()\n","ds = TDataset(texts, labels)\n","dl = DataLoader(ds, batch_size=128, shuffle=False, collate_fn=collator)\n","\n","for model_file in MODEL_FILES:\n","    print(f\"\\n=== Evaluating {model_file} ===\")\n","    model = BertWithDropout(\"bert-base-uncased\", num_labels=len(ORDERED_LABELS), dropout_rate=0.2)\n","\n","\n","    state = torch.load(os.path.join(MODEL_DIR, model_file), map_location=DEVICE)\n","    model.load_state_dict(state, strict=True)\n","    model.to(DEVICE)\n","\n","    metrics = evaluate_model(model, dl)\n","    print(f\"Acc={metrics['accuracy']:.4f}, F1(macro)={metrics['f1']:.4f}, \"\n","          f\"Precision={metrics['precision']:.4f}, Recall={metrics['recall']:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N0m_6EFSQ0um","executionInfo":{"status":"ok","timestamp":1755549130867,"user_tz":-180,"elapsed":14626,"user":{"displayName":"yarden shalom","userId":"10505379448441178663"}},"outputId":"1c597460-a6c1-4989-89d7-3c2542771521"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== Evaluating HF_best_model_stage3.pt ===\n","                    precision    recall  f1-score   support\n","\n","Extremely Negative     0.8961    0.8885    0.8923       592\n","          Negative     0.8814    0.8492    0.8650      1041\n","           Neutral     0.7828    0.8966    0.8358       619\n","          Positive     0.8552    0.8416    0.8483       947\n","Extremely Positive     0.9048    0.8564    0.8799       599\n","\n","          accuracy                         0.8623      3798\n","         macro avg     0.8640    0.8665    0.8643      3798\n","      weighted avg     0.8647    0.8623    0.8627      3798\n","\n","Acc=0.8623, F1(macro)=0.8643, Precision=0.8640, Recall=0.8665\n"]}]},{"cell_type":"markdown","source":["# Post-training Dynamic Quantization"],"metadata":{"id":"n81sSFuhfK6W"}},{"cell_type":"code","source":["DEVICE = torch.device(\"cpu\")\n","# Check\n","print(f\"Using device: {DEVICE}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zsbj1Z7JiGyd","executionInfo":{"status":"ok","timestamp":1755549370005,"user_tz":-180,"elapsed":9,"user":{"displayName":"yarden shalom","userId":"10505379448441178663"}},"outputId":"317940c8-dfb0-48ec-9cdf-d3771bca09ae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n"]}]},{"cell_type":"code","source":["# ======================\n","# Step 1: Robust loader for  Bert\n","# ======================\n","\n","import torch\n","import torch.nn as nn\n","from torch.quantization import quantize_dynamic\n","import os\n","\n","# ======================\n","# 1) Load your best Stage 3 model\n","# ======================\n","# --------- config ----------\n","MODEL_DIR  =data_dir\n","MODEL_REGISTRY = {\n","    \"HF_best_model_stage3.pt\":            (\"bert-base-uncased\",                                      BertWithDropout),\n","    \"best_BERT_base_uncasedmodel2.pt\":    (\"bert-base-uncased\",                                      ConfigurableBertModel),\n","}\n","\n","def _state_dict_size_mb(model) -> float:\n","    \"\"\"Compute saved state_dict size in MB.\"\"\"\n","    with tempfile.NamedTemporaryFile(delete=False, suffix=\".pt\") as tmp:\n","        torch.save(model.state_dict(), tmp.name)\n","        mb = os.path.getsize(tmp.name) / (1024**2)\n","    os.remove(tmp.name)\n","    return mb\n","\n","def load_model_for_file(model_file: str, num_labels: int = 5, dropout: float = 0.2):\n","    \"\"\"Instantiate the right class/backbone and load weights to CPU.\"\"\"\n","    if model_file not in MODEL_REGISTRY:\n","        raise ValueError(f\"Unknown model file: {model_file}\")\n","\n","    backbone, cls = MODEL_REGISTRY[model_file]\n","    model = cls(backbone, num_labels=num_labels, dropout_rate=dropout)\n","    state = torch.load(os.path.join(MODEL_DIR, model_file), map_location=\"cpu\")\n","    model.load_state_dict(state, strict=True)\n","    model.eval().cpu()\n","    return model, backbone\n","\n","print(\"Model                              | Backbone                                    | #Params        | state_dict (MB)\")\n","print(\"-\"*110)\n","for model_file in MODEL_REGISTRY.keys():\n","    model, backbone = load_model_for_file(model_file, num_labels=5, dropout=0.2)\n","    param_count = sum(p.numel() for p in model.parameters())\n","    size_mb = _state_dict_size_mb(model)\n","    print(f\"{model_file:34s} | {backbone:42s} | {param_count:>13,d} | {size_mb:>14.2f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BBGv1XPOfRKw","executionInfo":{"status":"ok","timestamp":1755462323124,"user_tz":-180,"elapsed":2345,"user":{"displayName":"yarden shalom","userId":"10505379448441178663"}},"outputId":"695be609-d1de-4981-c070-56283078ea0e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model                              | Backbone                                    | #Params        | state_dict (MB)\n","--------------------------------------------------------------------------------------------------------------\n","HF_best_model_stage3.pt            | bert-base-uncased                          |   109,486,085 |         417.73\n","best_BERT_base_uncasedmodel2.pt    | bert-base-uncased                          |   109,486,085 |         417.73\n"]}]},{"cell_type":"code","source":["# ======================\n","# 2) Apply Post-Training Dynamic Quantization for BOTH BERT models\n","# ======================\n","from torch.quantization import quantize_dynamic\n","\n","TARGET_FILES = [\n","    \"HF_best_model_stage3.pt\",\n","    \"best_BERT_base_uncasedmodel2.pt\",\n","]\n","\n","OUT_DIR = os.path.join(MODEL_DIR, \"quantized_int8\")\n","os.makedirs(OUT_DIR, exist_ok=True)\n","\n","for fname in TARGET_FILES:\n","    print(f\"\\n=== Quantizing: {fname} ===\")\n","    model, backbone = load_model_for_file(fname, num_labels=len(ordered_labels), dropout=0.2)\n","\n","    # sizes before/after\n","    size_fp32 = _state_dict_size_mb(model)\n","    quantized_model = quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)  # BERT: ok to quantize all Linear\n","    size_int8 = _state_dict_size_mb(quantized_model)\n","\n","    print(\"Quantization complete. Model is ready for inference.\")\n","    print(f\"State dict size: FP32={size_fp32:.2f} MB -> INT8={size_int8:.2f} MB\")\n","\n","    # ======================\n","    # 3) Save quantized model (unique name per file)\n","    # ======================\n","    quant_path = os.path.join(OUT_DIR, f\"{Path(fname).stem}_quantized_int8_sd.pt\")\n","    torch.save(quantized_model.state_dict(), quant_path)\n","    print(f\"Quantized model saved to: {quant_path}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wMUIlyP-fjUv","executionInfo":{"status":"ok","timestamp":1755462331451,"user_tz":-180,"elapsed":8324,"user":{"displayName":"yarden shalom","userId":"10505379448441178663"}},"outputId":"fce98275-e9d2-4ed1-fc37-ae9003b5dd87"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== Quantizing: HF_best_model_stage3.pt ===\n","Quantization complete. Model is ready for inference.\n","State dict size: FP32=417.73 MB -> INT8=173.09 MB\n","Quantized model saved to: /content/drive/MyDrive/deep_learning/quantized_int8/HF_best_model_stage3_quantized_int8_sd.pt\n","\n","=== Quantizing: best_BERT_base_uncasedmodel2.pt ===\n","Quantization complete. Model is ready for inference.\n","State dict size: FP32=417.73 MB -> INT8=173.09 MB\n","Quantized model saved to: /content/drive/MyDrive/deep_learning/quantized_int8/best_BERT_base_uncasedmodel2_quantized_int8_sd.pt\n"]}]},{"cell_type":"code","source":["# ==== INT8 quantization + evaluation for the TWO BERT models ====\n","import os, tempfile, torch, torch.nn as nn\n","from torch.quantization import quantize_dynamic\n","from transformers import AutoTokenizer, DataCollatorWithPadding\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n","\n","# Assumes these are already defined earlier in your notebook:\n","# - MODEL_DIR\n","# - ordered_labels\n","# - load_model_for_file(model_file, num_labels, dropout)\n","\n","# ---------- utilities ----------\n","import inspect\n","@torch.inference_mode()\n","def eval_int8_cpu(model, dataloader, label_names=None):\n","    \"\"\"Evaluate on CPU and print per-class report (passes token_type_ids only if supported).\"\"\"\n","    model.eval(); model.cpu()\n","    # detect which args the model.forward supports\n","    fwd_params = set(inspect.signature(model.forward).parameters.keys())\n","\n","    all_preds, all_labels = [], []\n","    for batch in dataloader:\n","        ids   = batch[\"input_ids\"]\n","        mask  = batch.get(\"attention_mask\")\n","        ttids = batch.get(\"token_type_ids\")\n","\n","        # build kwargs safely\n","        kwargs = {\"input_ids\": ids, \"attention_mask\": mask}\n","        if ttids is not None and \"token_type_ids\" in fwd_params:\n","            kwargs[\"token_type_ids\"] = ttids\n","\n","        out = model(**kwargs)\n","        logits = out[\"logits\"] if isinstance(out, dict) else out.logits\n","        preds  = torch.argmax(logits, dim=1).cpu().numpy()\n","        labels = batch.get(\"labels\", batch.get(\"label\")).cpu().numpy()\n","\n","        all_preds.extend(preds); all_labels.extend(labels)\n","\n","    # report\n","    print(classification_report(all_labels, all_preds, target_names=label_names, digits=4))\n","    metrics = {\n","        \"accuracy\":  accuracy_score(all_labels, all_preds),\n","        \"f1\":        f1_score(all_labels, all_preds, average=\"macro\"),\n","        \"precision\": precision_score(all_labels, all_preds, average=\"macro\", zero_division=0),\n","        \"recall\":    recall_score(all_labels, all_preds, average=\"macro\"),\n","    }\n","    print(f\"Accuracy={metrics['accuracy']:.4f} | F1(macro)={metrics['f1']:.4f} | \"\n","          f\"Precision(macro)={metrics['precision']:.4f} | Recall(macro)={metrics['recall']:.4f}\")\n","    return metrics\n","\n","\n","# ---------- build a BERT-specific test loader (tokenizer must match BERT) ----------\n","tok_bert = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True)\n","coll_bert = DataCollatorWithPadding(tokenizer=tok_bert)\n","\n","class _EvalBertDS(Dataset):\n","    def __init__(self, df, text_col=\"ProcessedTweet\", label_col=\"label\", max_len=128):\n","        self.texts  = df[text_col].astype(str).tolist()\n","        self.labels = df[label_col].astype(int).tolist()\n","        self.max_len = max_len\n","    def __len__(self): return len(self.texts)\n","    def __getitem__(self, i):\n","        enc = tok_bert(self.texts[i], truncation=True, max_length=self.max_len)\n","        enc[\"labels\"] = self.labels[i]\n","        return enc\n","\n","dl_bert = DataLoader(_EvalBertDS(test_df), batch_size=128, shuffle=False, collate_fn=coll_bert)\n","\n","# ---------- quantize + evaluate each BERT model ----------\n","bert_files = [\"HF_best_model_stage3.pt\", \"best_BERT_base_uncasedmodel2.pt\"]\n","\n","for MODEL_FILE in [\"HF_best_model_stage3.pt\", \"best_BERT_base_uncasedmodel2.pt\"]:\n","    model, _ = load_model_for_file(MODEL_FILE, num_labels=len(ordered_labels), dropout=0.2)\n","    q_model = quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)\n","    print(f\"\\nINT8 eval: {MODEL_FILE}\")\n","    _ = eval_int8_cpu(q_model, dl_bert, label_names=ordered_labels)  # או dl שמתאים ל-BERT\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n4GddBTfh7Un","executionInfo":{"status":"ok","timestamp":1755462472049,"user_tz":-180,"elapsed":140578,"user":{"displayName":"yarden shalom","userId":"10505379448441178663"}},"outputId":"450588ef-261a-4456-942d-f55328992a09"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","INT8 eval: HF_best_model_stage3.pt\n","                    precision    recall  f1-score   support\n","\n","Extremely Negative     0.9124    0.8091    0.8577       592\n","          Negative     0.8351    0.8223    0.8287      1041\n","           Neutral     0.7632    0.8853    0.8197       619\n","          Positive     0.8247    0.8194    0.8220       947\n","Extremely Positive     0.8591    0.8447    0.8519       599\n","\n","          accuracy                         0.8333      3798\n","         macro avg     0.8389    0.8362    0.8360      3798\n","      weighted avg     0.8366    0.8333    0.8337      3798\n","\n","Accuracy=0.8333 | F1(macro)=0.8360 | Precision(macro)=0.8389 | Recall(macro)=0.8362\n","\n","INT8 eval: best_BERT_base_uncasedmodel2.pt\n","                    precision    recall  f1-score   support\n","\n","Extremely Negative     0.8496    0.8970    0.8726       592\n","          Negative     0.8615    0.8425    0.8519      1041\n","           Neutral     0.9262    0.8514    0.8872       619\n","          Positive     0.8255    0.8194    0.8225       947\n","Extremely Positive     0.8313    0.8965    0.8627       599\n","\n","          accuracy                         0.8552      3798\n","         macro avg     0.8588    0.8613    0.8594      3798\n","      weighted avg     0.8564    0.8552    0.8552      3798\n","\n","Accuracy=0.8552 | F1(macro)=0.8594 | Precision(macro)=0.8588 | Recall(macro)=0.8613\n"]}]},{"cell_type":"markdown","source":["# unstructured pruning"],"metadata":{"id":"A9Je1Zdznw50"}},{"cell_type":"code","source":["# ========================================\n","# Global L1 Unstructured Pruning for BERT (clean E2E)\n","# ========================================\n","import os, tempfile, torch, torch.nn as nn\n","from torch.nn.utils import prune\n","\n","BEST_FILE = os.path.join(MODEL_DIR, \"HF_best_model_stage3.pt\")\n","PRUNED_SD = os.path.join(MODEL_DIR, \"HF_best_model_stage3_pruned40_sd.pt\")\n","\n","# --- 1) Loader for your trained model (FP32) ---\n","def load_fp32_model_state(path, num_labels):\n","    model = BertWithDropout(\"bert-base-uncased\", num_labels=num_labels, dropout_rate=0.2)\n","    state = torch.load(path, map_location=\"cpu\")\n","    model.load_state_dict(state, strict=True)\n","    model.eval().cpu()\n","    return model\n","\n","# --- 2) Utilities for pruning and reporting ---\n","def collect_linear_params(model, prune_classifier=False):\n","    \"\"\"Collect (module, 'weight') pairs for nn.Linear layers.\n","       If prune_classifier=False, keep the final classifier in FP32.\"\"\"\n","    params = []\n","    for name, module in model.named_modules():\n","        if isinstance(module, nn.Linear):\n","            if not prune_classifier and name.endswith(\"classifier\"):\n","                continue\n","            params.append((module, \"weight\"))\n","    return params\n","\n","def apply_global_l1(model, amount=0.4, prune_classifier=False):\n","    \"\"\"Global unstructured L1 pruning across selected Linear weights.\"\"\"\n","    targets = collect_linear_params(model, prune_classifier=prune_classifier)\n","    prune.global_unstructured(\n","        targets, pruning_method=prune.L1Unstructured, amount=amount\n","    )\n","    # Report density/sparsity on masked weights\n","    total, nonzero = 0, 0\n","    for m, _ in targets:\n","        w = m.weight  # masked view\n","        total   += w.numel()\n","        nonzero += (w != 0).sum().item()\n","    density  = nonzero / total if total else 1.0\n","    sparsity = 1.0 - density\n","    print(f\"Global L1 pruning applied: amount={amount:.2f} | density={density:.2%} | sparsity={sparsity:.2%}\")\n","    return targets, density, sparsity\n","\n","def remove_pruning_masks(targets):\n","    \"\"\"Make pruning permanent by removing mask reparametrizations.\"\"\"\n","    for m, _ in targets:\n","        prune.remove(m, \"weight\")\n","\n","def state_dict_size_mb(model):\n","    with tempfile.NamedTemporaryFile(delete=False, suffix=\".pt\") as tmp:\n","        torch.save(model.state_dict(), tmp.name)\n","        size_mb = os.path.getsize(tmp.name) / (1024**2)\n","    os.remove(tmp.name)\n","    return size_mb\n","\n","# --- 3) Load baseline model ---\n","model_pruned = load_fp32_model_state(BEST_FILE, num_labels=len(ORDERED_LABELS))\n","print(f\"Baseline state_dict size (dense): {state_dict_size_mb(model_pruned):.2f} MB\")\n","\n","# --- 4) Prune (recommend starting with 0.2–0.4) ---\n","targets, density, sparsity = apply_global_l1(model_pruned, amount=0.4, prune_classifier=False)\n","\n","# (Optional) quick fine-tune to recover accuracy could go here\n","\n","# --- 5) Make pruning permanent and save ---\n","remove_pruning_masks(targets)\n","print(f\"Pruned state_dict size (dense save): {state_dict_size_mb(model_pruned):.2f} MB\")\n","torch.save(model_pruned.state_dict(), PRUNED_SD)\n","print(\"Saved pruned state_dict to:\", PRUNED_SD)\n","\n","# --- 6) Evaluate pruned model on the SAME DataLoader `dl` ---\n","# Use your existing evaluate function; CPU is fine\n","pruned_metrics =eval_int8_cpu(model_pruned, dl)  # or: evaluate_model(model_pruned, dl, device=DEVICE)\n","print(f\"PRUNED -> Acc={pruned_metrics['accuracy']:.4f}, F1={pruned_metrics['f1']:.4f}, \"\n","      f\"P={pruned_metrics['precision']:.4f}, R={pruned_metrics['recall']:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8QU56yirjm2H","executionInfo":{"status":"ok","timestamp":1755377273755,"user_tz":-180,"elapsed":176167,"user":{"displayName":"yarden shalom","userId":"10505379448441178663"}},"outputId":"e791300e-7d47-40b2-d165-45394d0f3747"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Baseline state_dict size (dense): 417.73 MB\n","Global L1 pruning applied: amount=0.40 | density=60.00% | sparsity=40.00%\n","Pruned state_dict size (dense save): 417.73 MB\n","Saved pruned state_dict to: /content/drive/MyDrive/deep_learning/HF_best_model_stage3_pruned40_sd.pt\n","                    precision    recall  f1-score   support\n","\n","Extremely Negative     0.9030    0.8649    0.8835       592\n","          Negative     0.8762    0.8223    0.8484      1041\n","           Neutral     0.7547    0.9095    0.8249       619\n","          Positive     0.8479    0.8416    0.8447       947\n","Extremely Positive     0.9014    0.8548    0.8775       599\n","\n","          accuracy                         0.8531      3798\n","         macro avg     0.8566    0.8586    0.8558      3798\n","      weighted avg     0.8575    0.8531    0.8537      3798\n","\n","PRUNED -> Acc=0.8531, F1=0.8558, P=0.8566, R=0.8586\n"]}]},{"cell_type":"code","source":["# ==== INT8 quantization + evaluation for the TWO BERT models ====\n","import os, tempfile, torch, torch.nn as nn\n","from torch.quantization import quantize_dynamic\n","from transformers import AutoTokenizer, DataCollatorWithPadding\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n","\n","# Assumes these are already defined earlier in your notebook:\n","# - MODEL_DIR\n","# - ordered_labels\n","# - load_model_for_file(model_file, num_labels, dropout)\n","\n","# ---------- utilities ----------\n","import inspect\n","@torch.inference_mode()\n","def eval_int8_cpu(model, dataloader, label_names=None):\n","    \"\"\"Evaluate on CPU and print per-class report (passes token_type_ids only if supported).\"\"\"\n","    model.eval(); model.cpu()\n","    # detect which args the model.forward supports\n","    fwd_params = set(inspect.signature(model.forward).parameters.keys())\n","\n","    all_preds, all_labels = [], []\n","    for batch in dataloader:\n","        ids   = batch[\"input_ids\"]\n","        mask  = batch.get(\"attention_mask\")\n","        ttids = batch.get(\"token_type_ids\")\n","\n","        # build kwargs safely\n","        kwargs = {\"input_ids\": ids, \"attention_mask\": mask}\n","        if ttids is not None and \"token_type_ids\" in fwd_params:\n","            kwargs[\"token_type_ids\"] = ttids\n","\n","        out = model(**kwargs)\n","        logits = out[\"logits\"] if isinstance(out, dict) else out.logits\n","        preds  = torch.argmax(logits, dim=1).cpu().numpy()\n","        labels = batch.get(\"labels\", batch.get(\"label\")).cpu().numpy()\n","\n","        all_preds.extend(preds); all_labels.extend(labels)\n","\n","    # report\n","    print(classification_report(all_labels, all_preds, target_names=label_names, digits=4))\n","    metrics = {\n","        \"accuracy\":  accuracy_score(all_labels, all_preds),\n","        \"f1\":        f1_score(all_labels, all_preds, average=\"macro\"),\n","        \"precision\": precision_score(all_labels, all_preds, average=\"macro\", zero_division=0),\n","        \"recall\":    recall_score(all_labels, all_preds, average=\"macro\"),\n","    }\n","    print(f\"Accuracy={metrics['accuracy']:.4f} | F1(macro)={metrics['f1']:.4f} | \"\n","          f\"Precision(macro)={metrics['precision']:.4f} | Recall(macro)={metrics['recall']:.4f}\")\n","    return metrics\n","\n","\n","# ---------- build a BERT-specific test loader (tokenizer must match BERT) ----------\n","tok_bert = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True)\n","coll_bert = DataCollatorWithPadding(tokenizer=tok_bert)\n","\n","class _EvalBertDS(Dataset):\n","    def __init__(self, df, text_col=\"ProcessedTweet\", label_col=\"label\", max_len=128):\n","        self.texts  = df[text_col].astype(str).tolist()\n","        self.labels = df[label_col].astype(int).tolist()\n","        self.max_len = max_len\n","    def __len__(self): return len(self.texts)\n","    def __getitem__(self, i):\n","        enc = tok_bert(self.texts[i], truncation=True, max_length=self.max_len)\n","        enc[\"labels\"] = self.labels[i]\n","        return enc\n","\n","dl_bert = DataLoader(_EvalBertDS(test_df), batch_size=128, shuffle=False, collate_fn=coll_bert)"],"metadata":{"id":"eRCRiDDN5yMG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ==== FIX: load with key remapping (encoder. -> backbone. / bert. -> backbone.) ====\n","import re, os, tempfile, torch, torch.nn as nn\n","from torch.nn.utils import prune\n","\n","BEST_FILE = os.path.join(MODEL_DIR, \"best_BERT_base_uncasedmodel2.pt\")\n","PRUNED_SD = os.path.join(MODEL_DIR, \"best_BERT_base_uncasedmodel2_pruned25_ffn_sd.pt\")  # safer name\n","\n","def _remap_bert_keys(sd: dict) -> dict:\n","    new_sd = {}\n","    for k, v in sd.items():\n","        k2 = k\n","        if k2.startswith(\"encoder.\"):\n","            k2 = \"backbone.\" + k2[len(\"encoder.\"):]\n","        elif k2.startswith(\"bert.\"):\n","            k2 = \"backbone.\" + k2[len(\"bert.\"):]\n","        # optional old head names:\n","        k2 = k2.replace(\".cls.\", \".classifier.\")  # if you ever had 'cls' head\n","        new_sd[k2] = v\n","    return new_sd\n","\n","def load_fp32_model_state(path, num_labels):\n","    model = BertWithDropout(\"bert-base-uncased\", num_labels=num_labels, dropout_rate=0.2)\n","    raw_sd = torch.load(path, map_location=\"cpu\")\n","    remapped = _remap_bert_keys(raw_sd)\n","    missing, unexpected = model.load_state_dict(remapped, strict=False)\n","    if missing:\n","        print(\"WARN missing keys:\", missing)\n","    if unexpected:\n","        print(\"WARN unexpected keys:\", unexpected)\n","    model.eval().cpu()\n","    return model\n","\n","def state_dict_size_mb(model):\n","    with tempfile.NamedTemporaryFile(delete=False, suffix=\".pt\") as tmp:\n","        torch.save(model.state_dict(), tmp.name)\n","        size_mb = os.path.getsize(tmp.name) / (1024**2)\n","    os.remove(tmp.name)\n","    return size_mb\n","\n","# ==== SAFER PRUNING: FFN-only uniform (BERT) ====\n","def _collect_ffn_linear_params_only(model, prune_classifier=False):\n","    \"\"\"only intermediate.dense / output.dense under backbone.encoder.*\"\"\"\n","    targets = []\n","    for name, module in model.named_modules():\n","        if not isinstance(module, nn.Linear):\n","            continue\n","        if name.startswith(\"backbone.encoder\"):\n","            is_ffn = (\".intermediate.dense\" in name) or (\".output.dense\" in name)\n","            if is_ffn:\n","                targets.append((module, \"weight\"))\n","        elif prune_classifier and (\"classifier\" in name):\n","            targets.append((module, \"weight\"))\n","    return targets\n","\n","def apply_uniform_l1_ffn(model, amount_per_layer=0.25, prune_classifier=False):\n","    pairs = _collect_ffn_linear_params_only(model, prune_classifier)\n","    if not pairs:\n","        raise RuntimeError(\"No FFN Linear layers collected for pruning.\")\n","    for m, _ in pairs:\n","        prune.l1_unstructured(m, name=\"weight\", amount=amount_per_layer)\n","\n","    total, nonzero = 0, 0\n","    for m, _ in pairs:\n","        w = m.weight\n","        total += w.numel()\n","        nonzero += (w != 0).sum().item()\n","    density = nonzero / total if total else 1.0\n","    sparsity = 1.0 - density\n","    print(f\"Applied FFN-only uniform L1: per-layer amount={amount_per_layer:.2f} | \"\n","          f\"density={density:.2%} | sparsity={sparsity:.2%}\")\n","    return pairs\n","\n","def remove_pruning_masks(targets):\n","    for m, _ in targets:\n","        prune.remove(m, \"weight\")\n","\n","# ==== RUN ====\n","model_pruned = load_fp32_model_state(BEST_FILE, num_labels=len(ORDERED_LABELS))\n","print(f\"Baseline state_dict size (dense): {state_dict_size_mb(model_pruned):.2f} MB\")\n","\n","\n","targets = apply_uniform_l1_ffn(model_pruned, amount_per_layer=0.4, prune_classifier=False)# use safer FFN-only 0.25 first (you can try 0.20–0.25)\n","\n","remove_pruning_masks(targets)\n","print(f\"Pruned state_dict size (dense save): {state_dict_size_mb(model_pruned):.2f} MB\")\n","torch.save(model_pruned.state_dict(), PRUNED_SD)\n","print(\"Saved pruned state_dict to:\", PRUNED_SD)\n","\n","# ==== EVALUATE on your existing test loader ====\n","# Make sure you have evaluate_model_cpu that accepts token_type_ids safely (or just use yours)\n","pruned_metrics = eval_int8_cpu(model_pruned, test_loader_clean, label_names=ORDERED_LABELS)\n","print(f\"PRUNED -> Acc={pruned_metrics['accuracy']:.4f}, F1={pruned_metrics['f1']:.4f}, \"\n","      f\"P={pruned_metrics['precision']:.4f}, R={pruned_metrics['recall']:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sFfWfOas3d9i","executionInfo":{"status":"ok","timestamp":1755549625566,"user_tz":-180,"elapsed":110830,"user":{"displayName":"yarden shalom","userId":"10505379448441178663"}},"outputId":"2fbeaa64-82fb-4302-a93a-533ece8de2ff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Baseline state_dict size (dense): 417.73 MB\n","Applied FFN-only uniform L1: per-layer amount=0.40 | density=60.00% | sparsity=40.00%\n","Pruned state_dict size (dense save): 417.73 MB\n","Saved pruned state_dict to: /content/drive/MyDrive/deep_learning/best_BERT_base_uncasedmodel2_pruned25_ffn_sd.pt\n","                    precision    recall  f1-score   support\n","\n","Extremely Negative     0.8756    0.8801    0.8778       592\n","          Negative     0.8508    0.8598    0.8552      1041\n","           Neutral     0.9350    0.8595    0.8956       619\n","          Positive     0.8285    0.8363    0.8324       947\n","Extremely Positive     0.8482    0.8865    0.8669       599\n","\n","          accuracy                         0.8612      3798\n","         macro avg     0.8676    0.8644    0.8656      3798\n","      weighted avg     0.8624    0.8612    0.8615      3798\n","\n","Accuracy=0.8612 | F1(macro)=0.8656 | Precision(macro)=0.8676 | Recall(macro)=0.8644\n","PRUNED -> Acc=0.8612, F1=0.8656, P=0.8676, R=0.8644\n"]}]},{"cell_type":"markdown","source":["# Knowledge Distillation Demo"],"metadata":{"id":"Pz1_5z-roP2V"}},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments, AutoTokenizer, DataCollatorWithPadding\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# ===== Teacher: BERT base (Stage 3) =====\n","# If you already have a loader, replace this with it.\n","def load_fp32_teacher(path, num_labels):\n","    model = BertWithDropout(\"bert-base-uncased\", num_labels=num_labels, dropout_rate=0.2)\n","    state = torch.load(path, map_location=\"cpu\")          # expects state_dict\n","    model.load_state_dict(state, strict=True)\n","    model.eval()                                          # eval mode (dropout off)\n","    for p in model.parameters():\n","        p.requires_grad = False                           # fully frozen teacher\n","    return model\n","\n","teacher_ckpt =data_dir /\"HF_best_model_stage3.pt\"  # STAGE 3\n","teacher = load_fp32_teacher(teacher_ckpt, num_labels=len(ORDERED_LABELS)).to(device)\n","\n","# ===== Student: DistilBERT =====\n","student = AutoModelForSequenceClassification.from_pretrained(\n","    \"distilbert-base-uncased\",\n","    num_labels=len(ORDERED_LABELS)\n",").to(device)\n","\n","# (Optional, only if you need a tokenizer/collator here)\n","# tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\", use_fast=True)\n","# data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","# ===== Custom Trainer for KD =====\n","class DistillationTrainer(Trainer):\n","    def __init__(self, *args, teacher_model=None, temperature=2.0, alpha=0.7, **kwargs):\n","        \"\"\"\n","        alpha = weight for CE (hard labels); (1-alpha) = weight for KD (soft labels).\n","        temperature controls softening of distributions.\n","        \"\"\"\n","        super().__init__(*args, **kwargs)\n","        assert teacher_model is not None, \"teacher_model must be provided\"\n","        self.teacher = teacher_model\n","        self.temperature = temperature\n","        self.alpha = alpha\n","\n","    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n","        # Student forward (Trainer already moved inputs to device)\n","        outputs_student = model(**inputs)\n","        logits_student = outputs_student.logits\n","\n","        # Teacher forward (no grad, no token_type_ids)\n","        with torch.no_grad():\n","            t_outputs = self.teacher(\n","                input_ids=inputs[\"input_ids\"],\n","                attention_mask=inputs.get(\"attention_mask\", None)\n","            )\n","            logits_teacher = t_outputs[\"logits\"] if isinstance(t_outputs, dict) else t_outputs.logits\n","\n","        # Hard-label loss (ground truth)\n","        loss_ce = F.cross_entropy(logits_student, inputs[\"labels\"])\n","\n","        # Soft-label loss (teacher distillation)\n","        T = self.temperature\n","        loss_kl = F.kl_div(\n","            F.log_softmax(logits_student / T, dim=-1),\n","            F.softmax(logits_teacher / T, dim=-1),\n","            reduction=\"batchmean\"\n","        ) * (T ** 2)\n","\n","        loss = self.alpha * loss_ce + (1.0 - self.alpha) * loss_kl\n","        return (loss, outputs_student) if return_outputs else loss\n","\n","# ===== Training Args =====\n","training_args = TrainingArguments(\n","    output_dir=\"./distill_results_stage3_distilbert\",\n","    num_train_epochs=5,\n","    per_device_train_batch_size=32,\n","    per_device_eval_batch_size=64,\n","    eval_strategy=\"epoch\",           # <-- corrected\n","    save_strategy=\"epoch\",\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"f1_macro\",            # must match compute_metrics_fn keys\n","    greater_is_better=True,\n","    learning_rate=5e-5,\n","    weight_decay=0.01,\n","    logging_dir=\"./logs\",\n","    logging_steps=50,\n","    fp16=torch.cuda.is_available(),        # speed on GPU\n","    report_to=\"none\",\n",")\n","\n","# ===== Trainer =====\n","trainer_distill = DistillationTrainer(\n","    model=student,\n","    teacher_model=teacher,\n","    args=training_args,\n","    train_dataset=ds_tok[\"train\"],     # must return input_ids/attention_mask/labels\n","    eval_dataset=ds_tok[\"test\"],\n","    compute_metrics=compute_metrics_fn,\n","    # tokenizer=tokenizer,             # uncomment if you want Trainer-owned padding\n","    # data_collator=data_collator,\n",")\n","\n","# ===== Train =====\n","trainer_distill.train()\n","\n","# ===== Save student =====\n","trainer_distill.save_model(\"./distill_results_stage3_distilbert/best_student\")\n","print(\"\\nDistillation complete. Student (DistilBERT) model saved.\")\n","print(\"Student parameter count:\", sum(p.numel() for p in student.parameters()))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":339},"id":"cwSHpnBEoPhn","executionInfo":{"status":"ok","timestamp":1755378055912,"user_tz":-180,"elapsed":410733,"user":{"displayName":"yarden shalom","userId":"10505379448441178663"}},"outputId":"043ab71c-03a1-4deb-b429-2794b75f69bd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='5145' max='5145' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [5145/5145 06:48, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1 Micro</th>\n","      <th>F1 Macro</th>\n","      <th>F1 Weighted</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.998000</td>\n","      <td>0.914319</td>\n","      <td>0.802791</td>\n","      <td>0.802791</td>\n","      <td>0.811175</td>\n","      <td>0.802942</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.704500</td>\n","      <td>0.819215</td>\n","      <td>0.822012</td>\n","      <td>0.822012</td>\n","      <td>0.829159</td>\n","      <td>0.822848</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.437000</td>\n","      <td>0.805700</td>\n","      <td>0.838863</td>\n","      <td>0.838863</td>\n","      <td>0.846104</td>\n","      <td>0.839032</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.277500</td>\n","      <td>0.827808</td>\n","      <td>0.838073</td>\n","      <td>0.838073</td>\n","      <td>0.842795</td>\n","      <td>0.838088</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.215200</td>\n","      <td>0.880881</td>\n","      <td>0.840706</td>\n","      <td>0.840706</td>\n","      <td>0.844729</td>\n","      <td>0.840443</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Distillation complete. Student (DistilBERT) model saved.\n","Student parameter count: 66957317\n"]}]},{"cell_type":"code","source":["# --- Drop-in patch: handles models that don't accept token_type_ids (e.g., DistilBERT) ---\n","import inspect\n","import torch, numpy as np\n","from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score\n","\n","def _get_logits(outputs):\n","    return outputs[\"logits\"] if isinstance(outputs, dict) else outputs.logits\n","\n","def _supports_token_type_ids(model):\n","    try:\n","        return \"token_type_ids\" in inspect.signature(model.forward).parameters\n","    except (ValueError, AttributeError):\n","        return False\n","\n","@torch.inference_mode()\n","def eval_student(model, dataloader, device=\"cpu\", label_names=None):\n","    \"\"\"Evaluate a model on a DataLoader. Safe for both HF (no token_type_ids) and custom BERT (with token_type_ids).\"\"\"\n","    model.eval(); model.to(device)\n","    all_preds, all_labels = [], []\n","    use_ttids = _supports_token_type_ids(model)\n","\n","    for batch in dataloader:\n","        ids   = batch[\"input_ids\"].to(device)\n","        mask  = batch.get(\"attention_mask\")\n","        if mask is not None: mask = mask.to(device)\n","        ttids = batch.get(\"token_type_ids\")\n","        if ttids is not None: ttids = ttids.to(device)\n","        labels = batch.get(\"labels\", batch.get(\"label\")).to(device)\n","\n","        kwargs = {\"input_ids\": ids, \"attention_mask\": mask}\n","        if use_ttids and ttids is not None:\n","            kwargs[\"token_type_ids\"] = ttids\n","\n","        outputs = model(**kwargs)\n","        preds = torch.argmax(_get_logits(outputs), dim=-1)\n","\n","        all_preds.append(preds.cpu().numpy())\n","        all_labels.append(labels.cpu().numpy())\n","\n","    y_pred = np.concatenate(all_preds); y_true = np.concatenate(all_labels)\n","\n","    print(\"=== Classification Report ===\")\n","    print(classification_report(y_true, y_pred, target_names=label_names, digits=4) if label_names\n","          else classification_report(y_true, y_pred, digits=4))\n","\n","    metrics = {\n","        \"accuracy\":  accuracy_score(y_true, y_pred),\n","        \"f1\":        f1_score(y_true, y_pred, average=\"macro\"),\n","        \"precision\": precision_score(y_true, y_pred, average=\"macro\", zero_division=0),\n","        \"recall\":    recall_score(y_true, y_pred, average=\"macro\"),\n","    }\n","    print(f\"Accuracy={metrics['accuracy']:.4f} | F1(macro)={metrics['f1']:.4f} | \"\n","          f\"Precision(macro)={metrics['precision']:.4f} | Recall(macro)={metrics['recall']:.4f}\")\n","    return metrics\n"],"metadata":{"id":"4U3wxKunoPfI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Student FP32 (after KD)\n","stu_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","stu_metrics = eval_student(student, dl, device=stu_device, label_names=ORDERED_LABELS)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TRQJ4ZkYoPNl","executionInfo":{"status":"ok","timestamp":1755378378326,"user_tz":-180,"elapsed":2254,"user":{"displayName":"yarden shalom","userId":"10505379448441178663"}},"outputId":"033a7249-63da-4f9e-e182-7975641d6756"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["=== Classification Report ===\n","                    precision    recall  f1-score   support\n","\n","Extremely Negative     0.8433    0.8818    0.8621       592\n","          Negative     0.8252    0.8165    0.8209      1041\n","           Neutral     0.9190    0.8433    0.8795       619\n","          Positive     0.7864    0.8046    0.7954       947\n","Extremely Positive     0.8480    0.8664    0.8571       599\n","\n","          accuracy                         0.8360      3798\n","         macro avg     0.8444    0.8425    0.8430      3798\n","      weighted avg     0.8372    0.8360    0.8362      3798\n","\n","Accuracy=0.8360 | F1(macro)=0.8430 | Precision(macro)=0.8444 | Recall(macro)=0.8425\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","import numpy as np\n","from torch.utils.data import Dataset\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n","from transformers import (\n","    AutoTokenizer, DataCollatorWithPadding,\n","    AutoModelForSequenceClassification, Trainer, TrainingArguments, TrainerCallback\n",")\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","NUM_LABELS = len(ORDERED_LABELS)\n","\n","# ----------------------------\n","# Tokenizer/Collator (student)\n","# ----------------------------\n","STUDENT_CKPT = \"distilbert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(STUDENT_CKPT, use_fast=True)\n","collator  = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","# ----------------------------\n","# Dataset that uses the tokenizer above\n","# ----------------------------\n","class TextClsDataset(Dataset):\n","    def __init__(self, df, text_col: str, label_col: str, max_len: int = 128):\n","        self.texts  = df[text_col].astype(str).tolist()\n","        self.labels = df[label_col].astype(int).tolist()\n","        self.max_len = max_len\n","    def __len__(self): return len(self.texts)\n","    def __getitem__(self, i):\n","        enc = tokenizer(self.texts[i], truncation=True, max_length=self.max_len)\n","        enc[\"labels\"] = self.labels[i]\n","        return enc\n","\n","# Build train/val/test datasets (adjust text column if needed)\n","train_ds = TextClsDataset(train_df, text_col=\"ProcessedTweet\", label_col=\"label\")\n","val_ds   = TextClsDataset(eval_df,   text_col=\"ProcessedTweet\", label_col=\"label\")\n","test_ds  = TextClsDataset(test_df,   text_col=\"ProcessedTweet\", label_col=\"label\")\n","\n","# ----------------------------\n","# Teacher loader with key remapping\n","# ----------------------------\n","def _remap_bert_keys(sd: dict) -> dict:\n","    \"\"\"Map checkpoint keys saved as 'encoder.*' or 'bert.*' to 'backbone.*'.\n","       Also strips 'module.' if present, and normalizes '.cls.' -> '.classifier.'.\n","    \"\"\"\n","    new_sd = {}\n","    for k, v in sd.items():\n","        k2 = k\n","        if k2.startswith(\"module.\"):\n","            k2 = k2[len(\"module.\"):]\n","        if k2.startswith(\"encoder.\"):\n","            k2 = \"backbone.\" + k2[len(\"encoder.\"):]\n","        elif k2.startswith(\"bert.\"):\n","            k2 = \"backbone.\" + k2[len(\"bert.\"):]\n","        k2 = k2.replace(\".cls.\", \".classifier.\")\n","        new_sd[k2] = v\n","    return new_sd\n","\n","def load_fp32_teacher(path, num_labels):\n","    model = BertWithDropout(\"bert-base-uncased\", num_labels=num_labels, dropout_rate=0.2)\n","    raw_sd = torch.load(path, map_location=\"cpu\")\n","    remapped = _remap_bert_keys(raw_sd)\n","    missing, unexpected = model.load_state_dict(remapped, strict=False)\n","    if missing:    print(\"[WARN] missing keys:\", missing)\n","    if unexpected: print(\"[WARN] unexpected keys:\", unexpected)\n","    model.eval()\n","    for p in model.parameters():\n","        p.requires_grad = False\n","    return model\n","\n","teacher_ckpt = data_dir/\"best_BERT_base_uncasedmodel2.pt\"\n","teacher = load_fp32_teacher(teacher_ckpt, num_labels=NUM_LABELS).to(device)\n","\n","# ----------------------------\n","# Student: DistilBERT\n","# ----------------------------\n","student = AutoModelForSequenceClassification.from_pretrained(\n","    STUDENT_CKPT, num_labels=NUM_LABELS\n",").to(device)\n","\n","# ----------------------------\n","# Metrics (adds micro/weighted)\n","# ----------------------------\n","def compute_metrics_fn(eval_pred):\n","    preds  = getattr(eval_pred, \"predictions\", eval_pred[0])\n","    labels = getattr(eval_pred, \"label_ids\",    eval_pred[1])\n","    if isinstance(preds, tuple): preds = preds[0]\n","    preds = np.argmax(preds, axis=-1)\n","    return {\n","        \"accuracy\":      accuracy_score(labels, preds),\n","        \"f1_macro\":      f1_score(labels, preds, average=\"macro\"),\n","        \"f1_micro\":      f1_score(labels, preds, average=\"micro\"),\n","        \"f1_weighted\":   f1_score(labels, preds, average=\"weighted\"),\n","        \"precision\":     precision_score(labels, preds, average=\"macro\", zero_division=0),\n","        \"recall\":        recall_score(labels, preds, average=\"macro\"),\n","    }\n","\n","# ----------------------------\n","# KD Trainer\n","# ----------------------------\n","class DistillationTrainer(Trainer):\n","    def __init__(self, *args, teacher_model=None, temperature=2.0, alpha=0.7, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        assert teacher_model is not None, \"teacher_model must be provided\"\n","        self.teacher = teacher_model\n","        self.temperature = temperature\n","        self.alpha = alpha\n","\n","    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n","        # DistilBERT ignores token_type_ids; teacher BERT can use them if present\n","        s_inputs = dict(inputs)\n","        s_inputs.pop(\"token_type_ids\", None)\n","\n","        out_s = model(**s_inputs)\n","        logits_student = out_s.logits\n","\n","        with torch.no_grad():\n","            t_out = self.teacher(\n","                input_ids=s_inputs[\"input_ids\"],\n","                attention_mask=s_inputs.get(\"attention_mask\"),\n","                token_type_ids=inputs.get(\"token_type_ids\")  # safe: None if not present\n","            )\n","            logits_teacher = t_out[\"logits\"] if isinstance(t_out, dict) else t_out.logits\n","\n","        loss_ce = F.cross_entropy(logits_student, inputs[\"labels\"])\n","\n","        T = self.temperature\n","        loss_kl = F.kl_div(\n","            F.log_softmax(logits_student / T, dim=-1),\n","            F.softmax(logits_teacher / T, dim=-1),\n","            reduction=\"batchmean\"\n","        ) * (T ** 2)\n","\n","        loss = self.alpha * loss_ce + (1.0 - self.alpha) * loss_kl\n","        return (loss, out_s) if return_outputs else loss\n","\n","# ----------------------------\n","# Pretty per-epoch table (optional but handy)\n","# ----------------------------\n","class EpochTableCallback(TrainerCallback):\n","    def __init__(self):\n","        self._printed_header = False\n","        self.rows = []\n","    def _last_train_loss_for_epoch(self, state, epoch_int):\n","        for rec in reversed(state.log_history):\n","            if rec.get(\"epoch\") is None: continue\n","            if int(round(rec[\"epoch\"])) == epoch_int and \"loss\" in rec:\n","                return float(rec[\"loss\"])\n","        return float(\"nan\")\n","    def on_epoch_end(self, args, state, control, **kwargs):\n","        control.should_evaluate = True\n","    def on_evaluate(self, args, state, control, metrics, **kwargs):\n","        from math import isnan\n","        ep = int(round(metrics.get(\"epoch\", state.epoch or 0))) or (len(self.rows) + 1)\n","        tr_loss = self._last_train_loss_for_epoch(state, ep)\n","        row = {\n","            \"Epoch\": ep,\n","            \"Training Loss\": tr_loss,\n","            \"Validation Loss\": float(metrics.get(\"eval_loss\", \"nan\")),\n","            \"Accuracy\": float(metrics.get(\"eval_accuracy\", \"nan\")),\n","            \"F1 Micro\": float(metrics.get(\"eval_f1_micro\", \"nan\")),\n","            \"F1 Macro\": float(metrics.get(\"eval_f1_macro\", \"nan\")),\n","            \"F1 Weighted\": float(metrics.get(\"eval_f1_weighted\", \"nan\")),\n","        }\n","        self.rows.append(row)\n","        if not self._printed_header:\n","            print(\"\\nEpoch  Training Loss  Validation Loss   Accuracy   F1 Micro   F1 Macro  F1 Weighted\")\n","            self._printed_header = True\n","        def f(x):\n","            return \"   nan\" if (x != x) or isnan(x) else f\"{x:10.6f}\"\n","        print(f\"{row['Epoch']:>5}  {f(row['Training Loss'])}   {f(row['Validation Loss'])}  \"\n","              f\"{f(row['Accuracy'])}  {f(row['F1 Micro'])}  {f(row['F1 Macro'])}  {f(row['F1 Weighted'])}\")\n","\n","epoch_table = EpochTableCallback()\n","\n","# ----------------------------\n","# TrainingArguments\n","# ----------------------------\n","training_args = TrainingArguments(\n","    output_dir=\"./distill_results_best_BERT_base_uncasedmodel2\",\n","    num_train_epochs=5,\n","    per_device_train_batch_size=32,\n","    per_device_eval_batch_size=64,\n","    eval_strategy=\"epoch\",   # <- correct arg name\n","    save_strategy=\"epoch\",\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"f1_macro\",\n","    greater_is_better=True,\n","    learning_rate=5e-5,\n","    weight_decay=0.01,\n","    logging_dir=\"./logs\",\n","    logging_steps=50,\n","    fp16=torch.cuda.is_available(),\n","    report_to=\"none\",\n","    remove_unused_columns=False,\n",")\n","\n","# ----------------------------\n","# Trainer\n","# ----------------------------\n","trainer_distill = DistillationTrainer(\n","    model=student,\n","    teacher_model=teacher,\n","    args=training_args,\n","    train_dataset=train_ds,\n","    eval_dataset=val_ds,\n","    compute_metrics=compute_metrics_fn,\n","    data_collator=collator,\n",")\n","\n","trainer_distill.add_callback(epoch_table)\n","\n","# ----------------------------\n","# Train\n","# ----------------------------\n","trainer_distill.train()\n","\n","# ----------------------------\n","# End-of-run Classification Report on TEST\n","# ----------------------------\n","pred = trainer_distill.predict(test_ds)\n","y_true = pred.label_ids\n","y_pred = pred.predictions.argmax(axis=-1)\n","print(\"\\n=== Classification Report ===\")\n","print(classification_report(y_true, y_pred, target_names=ORDERED_LABELS, digits=4))\n","\n","# ----------------------------\n","# Save student\n","# ----------------------------\n","trainer_distill.save_model(\"./distill_results_best_BERT_base_uncasedmodel2/best_student\")\n","print(\"\\nDistillation complete. Student (DistilBERT) model saved.\")\n","print(\"Student parameter count:\", sum(p.numel() for p in student.parameters()))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":712},"id":"CKhrTuHRuRd_","executionInfo":{"status":"ok","timestamp":1755550779014,"user_tz":-180,"elapsed":357519,"user":{"displayName":"yarden shalom","userId":"10505379448441178663"}},"outputId":"a24d120c-a507-405e-9a41-25d057912808"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='5145' max='5145' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [5145/5145 05:50, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1 Macro</th>\n","      <th>F1 Micro</th>\n","      <th>F1 Weighted</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.776300</td>\n","      <td>0.616122</td>\n","      <td>0.827017</td>\n","      <td>0.832538</td>\n","      <td>0.827017</td>\n","      <td>0.826715</td>\n","      <td>0.828593</td>\n","      <td>0.840117</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.522000</td>\n","      <td>0.546772</td>\n","      <td>0.853134</td>\n","      <td>0.856823</td>\n","      <td>0.853134</td>\n","      <td>0.853718</td>\n","      <td>0.870272</td>\n","      <td>0.851172</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.290900</td>\n","      <td>0.515430</td>\n","      <td>0.860058</td>\n","      <td>0.863588</td>\n","      <td>0.860058</td>\n","      <td>0.860517</td>\n","      <td>0.873242</td>\n","      <td>0.859110</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.223400</td>\n","      <td>0.471406</td>\n","      <td>0.875364</td>\n","      <td>0.878759</td>\n","      <td>0.875364</td>\n","      <td>0.875434</td>\n","      <td>0.878200</td>\n","      <td>0.880345</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.131100</td>\n","      <td>0.497551</td>\n","      <td>0.875729</td>\n","      <td>0.878367</td>\n","      <td>0.875729</td>\n","      <td>0.875666</td>\n","      <td>0.875760</td>\n","      <td>0.881717</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Epoch  Training Loss  Validation Loss   Accuracy   F1 Micro   F1 Macro  F1 Weighted\n","    1    0.776300     0.616122    0.827017    0.827017    0.832538    0.826715\n","    2    0.522000     0.546772    0.853134    0.853134    0.856823    0.853718\n","    3    0.290900     0.515430    0.860058    0.860058    0.863588    0.860517\n","    4    0.223400     0.471406    0.875364    0.875364    0.878759    0.875434\n","    5    0.131100     0.497551    0.875729    0.875729    0.878367    0.875666\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","=== Classification Report ===\n","                    precision    recall  f1-score   support\n","\n","Extremely Negative     0.8535    0.9155    0.8835       592\n","          Negative     0.8617    0.8261    0.8436      1041\n","           Neutral     0.9135    0.8530    0.8822       619\n","          Positive     0.8084    0.8553    0.8312       947\n","Extremely Positive     0.8821    0.8614    0.8716       599\n","\n","          accuracy                         0.8573      3798\n","         macro avg     0.8638    0.8623    0.8624      3798\n","      weighted avg     0.8588    0.8573    0.8574      3798\n","\n","\n","Distillation complete. Student (DistilBERT) model saved.\n","Student parameter count: 66957317\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyMhnSIGzWq/NigDSQ+hJsFH"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"8c95e2cb5e4d4a10a83bd9defa312775":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b632f5376ff34502ade19db3347fb0ff","IPY_MODEL_f8337816508a408b8c0a768652890e08","IPY_MODEL_00871705415647cfa2384868f0eb33dc"],"layout":"IPY_MODEL_87910b9892c947169b312522083df579"}},"b632f5376ff34502ade19db3347fb0ff":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aabc15de5c0242ddada3f287b03d5699","placeholder":"​","style":"IPY_MODEL_1a563ff4992b424b8b066087dc36ce81","value":"tokenizer_config.json: 100%"}},"f8337816508a408b8c0a768652890e08":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_abda4339897b448ca23d4a16d9b282e0","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_60e252aa12884149acdce7c935f8e07c","value":48}},"00871705415647cfa2384868f0eb33dc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1822934776c74d288f51ee6c252deff1","placeholder":"​","style":"IPY_MODEL_a044273c11bb4d30ae37dec26fb257a3","value":" 48.0/48.0 [00:00&lt;00:00, 4.42kB/s]"}},"87910b9892c947169b312522083df579":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aabc15de5c0242ddada3f287b03d5699":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a563ff4992b424b8b066087dc36ce81":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"abda4339897b448ca23d4a16d9b282e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60e252aa12884149acdce7c935f8e07c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1822934776c74d288f51ee6c252deff1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a044273c11bb4d30ae37dec26fb257a3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"48ac50f872924ce2a8e55cb2c3f418ed":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_904a13e36c7d4db490adbd03da04dca6","IPY_MODEL_25f224dd8d0e48ca9a0038f3e2b38b15","IPY_MODEL_3ca8b9f99fba4a6bbc8246cb4ae6b4a5"],"layout":"IPY_MODEL_118df6fe52dd433393fe94013ea4a543"}},"904a13e36c7d4db490adbd03da04dca6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_37fdb3093fb14840b660b57a99c2a1f0","placeholder":"​","style":"IPY_MODEL_ca8a28e6d88f44cbb37f359adbe85b5e","value":"config.json: 100%"}},"25f224dd8d0e48ca9a0038f3e2b38b15":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_021b01e3e204444ca337de51a7a717b7","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8a0942347ee242ed80ce449a9444d752","value":570}},"3ca8b9f99fba4a6bbc8246cb4ae6b4a5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d9bcd9a56a34d35a4ccae050cbca4d1","placeholder":"​","style":"IPY_MODEL_4617ee04f4b24d5281959de6078d1798","value":" 570/570 [00:00&lt;00:00, 66.8kB/s]"}},"118df6fe52dd433393fe94013ea4a543":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37fdb3093fb14840b660b57a99c2a1f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca8a28e6d88f44cbb37f359adbe85b5e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"021b01e3e204444ca337de51a7a717b7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a0942347ee242ed80ce449a9444d752":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3d9bcd9a56a34d35a4ccae050cbca4d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4617ee04f4b24d5281959de6078d1798":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0aacb8bd53804d749767ff3c51962850":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_32bf877344724e29b0341a89a60b076b","IPY_MODEL_60962fd0f7894249bdd4f601ed57a9e5","IPY_MODEL_94d4e8f8a2b548e7b91575f25c61a188"],"layout":"IPY_MODEL_f9328d610f824ddbb1fabc36cf6a6e46"}},"32bf877344724e29b0341a89a60b076b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8898644770e946e7be3bd9a8555a39d6","placeholder":"​","style":"IPY_MODEL_a9b7d0bbb0034eb7aea3bd8987d0c8f6","value":"vocab.txt: 100%"}},"60962fd0f7894249bdd4f601ed57a9e5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d30a4ec32b41465c97b21d2095858f4a","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_505b443e9a1143c68791ac2629edf1a7","value":231508}},"94d4e8f8a2b548e7b91575f25c61a188":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a78dd164fa84840b3ca3d7cde6ec350","placeholder":"​","style":"IPY_MODEL_ce47ba4ee51f47d9a64056736d9d8809","value":" 232k/232k [00:00&lt;00:00, 5.33MB/s]"}},"f9328d610f824ddbb1fabc36cf6a6e46":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8898644770e946e7be3bd9a8555a39d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9b7d0bbb0034eb7aea3bd8987d0c8f6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d30a4ec32b41465c97b21d2095858f4a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"505b443e9a1143c68791ac2629edf1a7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3a78dd164fa84840b3ca3d7cde6ec350":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce47ba4ee51f47d9a64056736d9d8809":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e59c767d528b40c2b5356b7c77504095":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7bbbfb39f14943b4975712b3c7db4708","IPY_MODEL_ab87cd6113f447f9bd649959cbcce110","IPY_MODEL_09982295b35d41c1a39f9b6e2dbf89db"],"layout":"IPY_MODEL_b98a472d2dc0402b977646f02774e161"}},"7bbbfb39f14943b4975712b3c7db4708":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_735a89ee5f024047a725a0c0a7034446","placeholder":"​","style":"IPY_MODEL_5f211fb13d6e4ae1a5b9e35ade6d613d","value":"tokenizer.json: 100%"}},"ab87cd6113f447f9bd649959cbcce110":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a5400e92082a4683a9c15e105960f543","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a60e486faa644dae87d7946514af6b4d","value":466062}},"09982295b35d41c1a39f9b6e2dbf89db":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1b5d264de3ad49e4a7a2c0095267681f","placeholder":"​","style":"IPY_MODEL_8db4272b28c64830a331d0853bde081b","value":" 466k/466k [00:00&lt;00:00, 27.3MB/s]"}},"b98a472d2dc0402b977646f02774e161":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"735a89ee5f024047a725a0c0a7034446":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f211fb13d6e4ae1a5b9e35ade6d613d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a5400e92082a4683a9c15e105960f543":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a60e486faa644dae87d7946514af6b4d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1b5d264de3ad49e4a7a2c0095267681f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8db4272b28c64830a331d0853bde081b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e11787fde0d94c2696789ea5b1c22fc8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6997aaf43c0d48879e578183d5d32f2d","IPY_MODEL_2f7543cf98fe4979abd518f461fb077d","IPY_MODEL_ed14bd90d6b5432f80225d6ef469ae0b"],"layout":"IPY_MODEL_eaea1b14c5af4484b2c93b3cdcfbb7b3"}},"6997aaf43c0d48879e578183d5d32f2d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5eff5de808c54ed89ac1418c1314cde1","placeholder":"​","style":"IPY_MODEL_c552136156594ad89b2e57752ac9f374","value":"model.safetensors: 100%"}},"2f7543cf98fe4979abd518f461fb077d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0cded3539ab9412192ebb5c99ad42591","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_16338d707b4d48e0a69ce2f6674e71fb","value":440449768}},"ed14bd90d6b5432f80225d6ef469ae0b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9586bb098b4d495c82ed9d9ebb4dba35","placeholder":"​","style":"IPY_MODEL_ef553f8f64f14074bbe4fd9a9642a129","value":" 440M/440M [00:01&lt;00:00, 407MB/s]"}},"eaea1b14c5af4484b2c93b3cdcfbb7b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5eff5de808c54ed89ac1418c1314cde1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c552136156594ad89b2e57752ac9f374":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0cded3539ab9412192ebb5c99ad42591":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16338d707b4d48e0a69ce2f6674e71fb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9586bb098b4d495c82ed9d9ebb4dba35":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef553f8f64f14074bbe4fd9a9642a129":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6557e17dc96c43a08a56658571ba913b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c7aa49f7b2874b55b4d30c37fdb4531c","IPY_MODEL_2ecf254d59ba42b5a336367987ee4e73","IPY_MODEL_8ec1bf9c437c420eaf7c941a6c141dfa"],"layout":"IPY_MODEL_ed825e4e3f964d4897e65fb9ef624d09"}},"c7aa49f7b2874b55b4d30c37fdb4531c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_97573ee09eee4f15bacd72ef8c4f38b9","placeholder":"​","style":"IPY_MODEL_83ac8c35c8ce4bcb8d9c0d723b7c1449","value":"config.json: 100%"}},"2ecf254d59ba42b5a336367987ee4e73":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8b832ac05ec7479cb3f23e862d0d9b03","max":929,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c0da58b8606243b6a5ed1f703988a23e","value":929}},"8ec1bf9c437c420eaf7c941a6c141dfa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e2e8e6efe5d4096880b932adb401833","placeholder":"​","style":"IPY_MODEL_c74ef0c3c7af496680d9f6ca54b66133","value":" 929/929 [00:00&lt;00:00, 78.4kB/s]"}},"ed825e4e3f964d4897e65fb9ef624d09":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"97573ee09eee4f15bacd72ef8c4f38b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"83ac8c35c8ce4bcb8d9c0d723b7c1449":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8b832ac05ec7479cb3f23e862d0d9b03":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0da58b8606243b6a5ed1f703988a23e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7e2e8e6efe5d4096880b932adb401833":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c74ef0c3c7af496680d9f6ca54b66133":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b666c1d18ca944cda298d3af0a6e2c04":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_90cdacfce6534f9c8d9d997c31002c98","IPY_MODEL_0e0fe4e2e4af4fe4b4c710a2d99c3dc3","IPY_MODEL_34515c1265194c22aed09047f8f2a7e1"],"layout":"IPY_MODEL_065d66fa529d432d8b91a567099d7935"}},"90cdacfce6534f9c8d9d997c31002c98":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6d1d512b9278466aa728a98473069b5f","placeholder":"​","style":"IPY_MODEL_7e701d0514b04ba88eedc53091926459","value":"vocab.json: "}},"0e0fe4e2e4af4fe4b4c710a2d99c3dc3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7ef311adef084b8c9ff8ecc4d1d41d95","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_69ffe4ae7f7243d8a7f9c8d5c4543ba4","value":1}},"34515c1265194c22aed09047f8f2a7e1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_18a6bf95105349a88d2cf73cf236ea60","placeholder":"​","style":"IPY_MODEL_48388792736f4f4ca6aac639b173da59","value":" 899k/? [00:00&lt;00:00, 42.5MB/s]"}},"065d66fa529d432d8b91a567099d7935":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d1d512b9278466aa728a98473069b5f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e701d0514b04ba88eedc53091926459":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7ef311adef084b8c9ff8ecc4d1d41d95":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"69ffe4ae7f7243d8a7f9c8d5c4543ba4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"18a6bf95105349a88d2cf73cf236ea60":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48388792736f4f4ca6aac639b173da59":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d1b62b51ad694819b49402fb5c60dbb5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c2c0e9e9ddab4593b0d0f43342fef8c4","IPY_MODEL_9a9c100dbb234f8b93c5a8197a1af0c0","IPY_MODEL_1987b62301e442a49f878a9396247c58"],"layout":"IPY_MODEL_aee670aaf4f54071ad945d83fd44019f"}},"c2c0e9e9ddab4593b0d0f43342fef8c4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_671a378e1e924b5ea6cb8c557cf63f9c","placeholder":"​","style":"IPY_MODEL_75e7e98d2e0f4fbe869b21862b66ab16","value":"merges.txt: "}},"9a9c100dbb234f8b93c5a8197a1af0c0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_23827a28f24a4bb29154bd001ed4d95e","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7599c511526e4597b9ffd2d6a9bf7bc5","value":1}},"1987b62301e442a49f878a9396247c58":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_693a164af51540debcfc43e9108f4abd","placeholder":"​","style":"IPY_MODEL_b891b3c2c70f478695f40741bb14e7bd","value":" 456k/? [00:00&lt;00:00, 29.7MB/s]"}},"aee670aaf4f54071ad945d83fd44019f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"671a378e1e924b5ea6cb8c557cf63f9c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75e7e98d2e0f4fbe869b21862b66ab16":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"23827a28f24a4bb29154bd001ed4d95e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"7599c511526e4597b9ffd2d6a9bf7bc5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"693a164af51540debcfc43e9108f4abd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b891b3c2c70f478695f40741bb14e7bd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"227aa49f05454ab0878c2ae582fc5c54":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_45ca09ebeb024678ade5cef952302fd3","IPY_MODEL_c01fc89589d246f59e1fb3744b7b7c39","IPY_MODEL_d6b9fcccdfba422b934d961c230f7b70"],"layout":"IPY_MODEL_49e16c630eeb45b497599954df9f2aa0"}},"45ca09ebeb024678ade5cef952302fd3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_775fb8b9636c4ee885ba27031fe55601","placeholder":"​","style":"IPY_MODEL_43161b51338642d0b72d9c0d90eff1db","value":"special_tokens_map.json: 100%"}},"c01fc89589d246f59e1fb3744b7b7c39":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d9f2139a490f4a3591b4f5f2f3995045","max":239,"min":0,"orientation":"horizontal","style":"IPY_MODEL_627b1077b52242b19e2e2a6b49b7e0c2","value":239}},"d6b9fcccdfba422b934d961c230f7b70":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1cd7aa72cfcb48649465620c619e4e78","placeholder":"​","style":"IPY_MODEL_9e0479fd544b4e4a9124e91a664141fb","value":" 239/239 [00:00&lt;00:00, 21.7kB/s]"}},"49e16c630eeb45b497599954df9f2aa0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"775fb8b9636c4ee885ba27031fe55601":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"43161b51338642d0b72d9c0d90eff1db":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d9f2139a490f4a3591b4f5f2f3995045":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"627b1077b52242b19e2e2a6b49b7e0c2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1cd7aa72cfcb48649465620c619e4e78":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9e0479fd544b4e4a9124e91a664141fb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}